{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "b5A5tEQlo_qU"
      },
      "outputs": [],
      "source": [
        "# !pip install wandb -qU\n",
        "# import wandb\n",
        "# wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-RqFIM-o_qb",
        "outputId": "ecde4ce2-d4d3-471b-ae05-5a1061b6c2cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in c:\\python311\\lib\\site-packages (2.3.0)\n",
            "Requirement already satisfied: torchvision in c:\\python311\\lib\\site-packages (0.18.0)\n",
            "Requirement already satisfied: torchaudio in c:\\python311\\lib\\site-packages (2.3.0)\n",
            "Requirement already satisfied: filelock in c:\\python311\\lib\\site-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\python311\\lib\\site-packages (from torch) (4.12.1)\n",
            "Requirement already satisfied: sympy in c:\\python311\\lib\\site-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in c:\\python311\\lib\\site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\python311\\lib\\site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\python311\\lib\\site-packages (from torch) (2024.6.0)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\python311\\lib\\site-packages (from torch) (2021.4.0)\n",
            "Requirement already satisfied: numpy in c:\\python311\\lib\\site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\python311\\lib\\site-packages (from torchvision) (10.3.0)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\python311\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\python311\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.12.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python311\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zXcLBaj8o_qb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gkcPuNcFo_qc"
      },
      "outputs": [],
      "source": [
        "# Siamese Network\n",
        "class TinySiameseNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TinySiameseNetwork, self).__init__()\n",
        "        self.conv_net = nn.Sequential(\n",
        "            nn.Conv2d(1, 2, kernel_size=3, stride=1, padding=1), # double to (1,4...)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(2, 4, kernel_size=3, stride=1, padding=1), # double to (4,8...)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(4, 8, kernel_size=3, stride=1, padding=1), # double to (4,8...)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(8, 4, kernel_size=3, stride=1, padding=1), # double to (4,8...)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(4, 2, kernel_size=3, stride=1, padding=1), # double to (4,8...)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(2*28*28, 8), # double to (8*28*28, 16)\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(8, 1), # double to (16,1)\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward_once(self, x):\n",
        "        output = self.conv_net(x)\n",
        "        output = output.view(output.size()[0], -1)\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        output1 = self.forward_once(img1)\n",
        "        output2 = self.forward_once(img2)\n",
        "        return torch.abs(output1 - output2)\n",
        "\n",
        "\n",
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=0)\n",
        "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=0)\n",
        "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=0)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(32 * 12 * 12, 30)  # Updated to 32 * 12 * 12\n",
        "        self.fc2 = nn.Linear(30, 15)\n",
        "        self.fc3 = nn.Linear(15, 1)\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)  # output size: (8, 55, 55)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)  # output size: (16, 26, 26)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.max_pool2d(x, 2)  # output size: (32, 12, 12)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        output1 = self.forward_one(input1)\n",
        "        output2 = self.forward_one(input2)\n",
        "        distance = torch.abs(output1 - output2)\n",
        "        output = torch.sigmoid(self.fc3(distance))\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bvxdb-lgo_qc",
        "outputId": "bd5c4c58-3a4e-400d-d17b-b23b8d28f4eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size: 128\n",
            "LR: 0.01\n",
            "Epochs: 5\n",
            "Device: cpu\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 8, 110, 110]              80\n",
            "            Conv2d-2           [-1, 16, 53, 53]           1,168\n",
            "            Conv2d-3           [-1, 32, 24, 24]           4,640\n",
            "            Linear-4                   [-1, 30]         138,270\n",
            "            Linear-5                   [-1, 15]             465\n",
            "            Conv2d-6          [-1, 8, 110, 110]              80\n",
            "            Conv2d-7           [-1, 16, 53, 53]           1,168\n",
            "            Conv2d-8           [-1, 32, 24, 24]           4,640\n",
            "            Linear-9                   [-1, 30]         138,270\n",
            "           Linear-10                   [-1, 15]             465\n",
            "           Linear-11                    [-1, 1]              16\n",
            "================================================================\n",
            "Total params: 289,262\n",
            "Trainable params: 289,262\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 600.25\n",
            "Forward/backward pass size (MB): 2.44\n",
            "Params size (MB): 1.10\n",
            "Estimated Total Size (MB): 603.80\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5: 0batch [00:00, ?batch/s]\n"
          ]
        },
        {
          "ename": "ZeroDivisionError",
          "evalue": "division by zero",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[15], line 137\u001b[0m\n\u001b[0;32m    134\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# Evaluate on test set\u001b[39;00m\n\u001b[0;32m    140\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader, criterion)\n",
            "Cell \u001b[1;32mIn[15], line 97\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[0;32m     94\u001b[0m         pbar\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mrunning_loss \u001b[38;5;241m/\u001b[39m (pbar\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     95\u001b[0m         pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 97\u001b[0m val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[15], line 77\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, data_loader, criterion)\u001b[0m\n\u001b[0;32m     75\u001b[0m         correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m label)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     76\u001b[0m         total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 77\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mcorrect\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_loader), accuracy\n",
            "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "class FaceDataset(Dataset):\n",
        "    def __init__(self, image_folder, transform=None):\n",
        "        self.image_folder = image_folder\n",
        "        self.transform = transform\n",
        "        self.image_pairs = []\n",
        "        self.labels = []\n",
        "        self._prepare_data()\n",
        "\n",
        "    def _prepare_data(self):\n",
        "        people_dirs = os.listdir(self.image_folder)\n",
        "        #for person_dir in people_dirs:\n",
        "        for i in range(1, int(len(people_dirs) * 0.4)):\n",
        "            person_dir = people_dirs[i]\n",
        "            person_path = os.path.join(self.image_folder, person_dir)\n",
        "            images = os.listdir(person_path)\n",
        "            for i in range(len(images)):\n",
        "                for j in range(i + 1, len(images)):\n",
        "                    self.image_pairs.append((os.path.join(person_path, images[i]), os.path.join(person_path, images[j])))\n",
        "                    self.labels.append(1)\n",
        "\n",
        "                    # Add negative samples\n",
        "                    neg_person = person_dir\n",
        "                    while neg_person == person_dir:\n",
        "                        neg_person = people_dirs[torch.randint(len(people_dirs), (1,)).item()]\n",
        "\n",
        "                    neg_images = os.listdir(os.path.join(self.image_folder, neg_person))\n",
        "                    self.image_pairs.append((os.path.join(person_path, images[i]), os.path.join(self.image_folder, neg_person, neg_images[0])))\n",
        "                    self.labels.append(0)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img1_path, img2_path = self.image_pairs[idx]\n",
        "        label = self.labels[idx]\n",
        "        img1 = Image.open(img1_path).convert('L')\n",
        "        img2 = Image.open(img2_path).convert('L')\n",
        "\n",
        "        if self.transform:\n",
        "            img1 = self.transform(img1)\n",
        "            img2 = self.transform(img2)\n",
        "\n",
        "        return img1, img2, torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "# Split dataset into training, validation, and test sets\n",
        "def split_dataset(dataset, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
        "    total_len = len(dataset)\n",
        "    indices = list(range(total_len))\n",
        "    random.shuffle(indices)\n",
        "\n",
        "    train_end = int(train_ratio * total_len)\n",
        "    val_end = train_end + int(val_ratio * total_len)\n",
        "\n",
        "    train_indices = indices[:train_end]\n",
        "    val_indices = indices[train_end:val_end]\n",
        "    test_indices = indices[val_end:]\n",
        "\n",
        "    return train_indices, val_indices, test_indices\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for img1, img2, label in data_loader:\n",
        "            img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
        "            outputs = model(img1, img2).squeeze()\n",
        "            loss = criterion(outputs, label)\n",
        "            running_loss += loss.item()\n",
        "            predicted = (outputs > 0.5).float()\n",
        "            correct += (predicted == label).sum().item()\n",
        "            total += label.size(0)\n",
        "    accuracy = correct / total\n",
        "    return running_loss / len(data_loader), accuracy\n",
        "\n",
        "# Training script with validation\n",
        "def train(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\") as pbar:\n",
        "            for img1, img2, label in train_loader:\n",
        "                img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(img1, img2).squeeze()\n",
        "                loss = criterion(outputs, label)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "                pbar.set_postfix(loss=running_loss / (pbar.n + 1))\n",
        "                pbar.update(1)\n",
        "\n",
        "        val_loss, val_accuracy = evaluate(model, val_loader, criterion)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {running_loss/len(train_loader)}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}\")\n",
        "\n",
        "        # Save the model\n",
        "        torch.save(model.state_dict(), f'networks/network_epoch{epoch}.pth')\n",
        "\n",
        "# Hyperparameters and setup\n",
        "batch_size = 128\n",
        "learning_rate = 0.01\n",
        "epochs = 5\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f'Batch size: {batch_size}')\n",
        "print(f'LR: {learning_rate}')\n",
        "print(f'Epochs: {epochs}')\n",
        "print(f'Device: {device}')\n",
        "\n",
        "# Data augmentation and normalization\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((112, 112)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# # Load dataset\n",
        "# image_folder = '/content/my_data/generated_images_10Kids_cropped'  # Update with the path to your dataset\n",
        "image_folder = './generated_images_10Kids_cropped'  # Update with the path to your dataset\n",
        "dataset = FaceDataset(image_folder, transform=transform)\n",
        "train_indices, val_indices, test_indices = split_dataset(dataset)\n",
        "\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, sampler=torch.utils.data.SubsetRandomSampler(train_indices))\n",
        "val_loader = DataLoader(dataset, batch_size=batch_size, sampler=torch.utils.data.SubsetRandomSampler(val_indices))\n",
        "test_loader = DataLoader(dataset, batch_size=batch_size, sampler=torch.utils.data.SubsetRandomSampler(test_indices))\n",
        "\n",
        "# Model, loss, and optimizer\n",
        "model = SiameseNetwork().to(device)\n",
        "summary(model, [(1, 112, 112), (1, 112, 112)])\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "train(model, train_loader, val_loader, criterion, optimizer, epochs=epochs)\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_accuracy = evaluate(model, test_loader, criterion)\n",
        "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
