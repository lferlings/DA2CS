{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "b5A5tEQlo_qU"
      },
      "outputs": [],
      "source": [
        "# !pip install wandb -qU\n",
        "# import wandb\n",
        "# wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PG3-_ByI0xF9",
        "outputId": "0c40471f-0ba2-40f6-fd16-267573a63207"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # # Unzip data\n",
        "# !unzip /content/drive/MyDrive/generated_images_10Kids_cropped.zip -d my_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-RqFIM-o_qb",
        "outputId": "97540303-32b8-468d-8d88-7b8c2873526f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zXcLBaj8o_qb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ioKil3xi95PM"
      },
      "outputs": [],
      "source": [
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(8)\n",
        "        self.conv2 = nn.Conv2d(8, 16, kernel_size=5, padding=0)\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=0)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(32 * 12 * 12, 41)  # Updated to 32 * 12 * 12\n",
        "        self.fc1komma5 = nn.Linear(41,32)\n",
        "        self.fc2 = nn.Linear(32, 16)\n",
        "        self.fc3 = nn.Linear(16, 1)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        x = F.relu(self.conv1(x)) # 8 * 112 * 112\n",
        "        x = F.max_pool2d(x, 2)  # output size: (8, 56, 56)\n",
        "        x = F.relu(self.conv2(x)) # 16* 52 * 52\n",
        "        x = F.max_pool2d(x, 2)  # output size: (16, 26, 26)\n",
        "        x = F.relu(self.conv3(x)) # 32 * 24 * 24\n",
        "        x = F.max_pool2d(x, 2)  # output size: (32, 12, 12)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc1komma5(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        output1 = self.forward_one(input1)\n",
        "        output2 = self.forward_one(input2)\n",
        "        distance = torch.abs(output1 - output2)\n",
        "        output = torch.sigmoid(self.fc3(distance))\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bvxdb-lgo_qc",
        "outputId": "78171c77-1c04-4e9a-c2b9-ebe6f268635f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size: 128\n",
            "LR: 0.01\n",
            "Epochs: 5\n",
            "Device: cpu\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/generated_images_10Kids_cropped'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 85\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m     84\u001b[0m image_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/generated_images_10Kids_cropped\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Update with the path to your dataset\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m train_dirs, val_dirs, test_dirs \u001b[38;5;241m=\u001b[39m \u001b[43msplit_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m FaceDataset(image_folder, train_dirs, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[1;32m     88\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m FaceDataset(image_folder, val_dirs, transform\u001b[38;5;241m=\u001b[39mtransform)\n",
            "Cell \u001b[0;32mIn[18], line 54\u001b[0m, in \u001b[0;36msplit_dataset\u001b[0;34m(image_folder, train_ratio, val_ratio, test_ratio)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit_dataset\u001b[39m(image_folder, train_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, val_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.15\u001b[39m, test_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.15\u001b[39m):\n\u001b[0;32m---> 54\u001b[0m     people_dirs \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     random\u001b[38;5;241m.\u001b[39mshuffle(people_dirs)\n\u001b[1;32m     57\u001b[0m     train_end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(train_ratio \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(people_dirs))\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/generated_images_10Kids_cropped'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "class FaceDataset(Dataset):\n",
        "    def __init__(self, image_folder, people_dirs, transform=None):\n",
        "        self.image_folder = image_folder\n",
        "        self.people_dirs = people_dirs\n",
        "        self.transform = transform\n",
        "        self.image_pairs = []\n",
        "        self.labels = []\n",
        "        self._prepare_data()\n",
        "\n",
        "    def _prepare_data(self):\n",
        "        for person_dir in self.people_dirs:\n",
        "            person_path = os.path.join(self.image_folder, person_dir)\n",
        "            images = os.listdir(person_path)\n",
        "            for i in range(len(images)):\n",
        "                for j in range(i + 1, len(images)):\n",
        "                    self.image_pairs.append((os.path.join(person_path, images[i]), os.path.join(person_path, images[j])))\n",
        "                    self.labels.append(1)\n",
        "\n",
        "                    # Add negative samples\n",
        "                    neg_person = person_dir\n",
        "                    while neg_person == person_dir:\n",
        "                        neg_person = random.choice(self.people_dirs)\n",
        "\n",
        "                    neg_images = os.listdir(os.path.join(self.image_folder, neg_person))\n",
        "                    random_image_index = random.randrange(start=0, stop=len(neg_images))\n",
        "                    self.image_pairs.append((os.path.join(person_path, images[i]), os.path.join(self.image_folder, neg_person, neg_images[random_image_index])))\n",
        "                    self.labels.append(0)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img1_path, img2_path = self.image_pairs[idx]\n",
        "        label = self.labels[idx]\n",
        "        img1 = Image.open(img1_path).convert('L')\n",
        "        img2 = Image.open(img2_path).convert('L')\n",
        "\n",
        "        if self.transform:\n",
        "            img1 = self.transform(img1)\n",
        "            img2 = self.transform(img2)\n",
        "\n",
        "        return img1, img2, torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "# Function to split dataset\n",
        "def split_dataset(image_folder, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
        "    people_dirs = os.listdir(image_folder)\n",
        "    random.shuffle(people_dirs)\n",
        "\n",
        "    train_end = int(train_ratio * len(people_dirs))\n",
        "    val_end = train_end + int(val_ratio * len(people_dirs))\n",
        "\n",
        "    train_dirs = people_dirs[:train_end]\n",
        "    val_dirs = people_dirs[train_end:val_end]\n",
        "    test_dirs = people_dirs[val_end:]\n",
        "\n",
        "    return train_dirs, val_dirs, test_dirs\n",
        "\n",
        "# Hyperparameters and setup\n",
        "batch_size = 128\n",
        "learning_rate = 0.01\n",
        "epochs = 5\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f'Batch size: {batch_size}')\n",
        "print(f'LR: {learning_rate}')\n",
        "print(f'Epochs: {epochs}')\n",
        "print(f'Device: {device}')\n",
        "\n",
        "# Data augmentation and normalization\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((112, 112)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Load dataset\n",
        "image_folder = 'generated_images_10Kids_cropped'  # Update with the path to your dataset\n",
        "train_dirs, val_dirs, test_dirs = split_dataset(image_folder)\n",
        "\n",
        "train_dataset = FaceDataset(image_folder, train_dirs, transform=transform)\n",
        "val_dataset = FaceDataset(image_folder, val_dirs, transform=transform)\n",
        "test_dataset = FaceDataset(image_folder, test_dirs, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Model, loss, and optimizer\n",
        "model = SiameseNetwork().to(device)\n",
        "summary(model, [(1, 112, 112), (1, 112, 112)])\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "# Training script with validation\n",
        "def train(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\") as pbar:\n",
        "            for img1, img2, label in train_loader:\n",
        "                img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(img1, img2).squeeze()\n",
        "                loss = criterion(outputs, label)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "                pbar.set_postfix(loss=running_loss / (pbar.n + 1))\n",
        "                pbar.update(1)\n",
        "        scheduler.step()\n",
        "        val_loss, val_accuracy = evaluate(model, val_loader, criterion)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {running_loss/len(train_loader)}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}\")\n",
        "\n",
        "        # Save the model\n",
        "        torch.save(model.state_dict(), f'networks/network_epoch{epoch}.pth')\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for img1, img2, label in data_loader:\n",
        "            img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
        "            outputs = model(img1, img2).squeeze()\n",
        "            loss = criterion(outputs, label)\n",
        "            running_loss += loss.item()\n",
        "            predicted = (outputs > 0.5).float()\n",
        "            correct += (predicted == label).sum().item()\n",
        "            total += label.size(0)\n",
        "    accuracy = correct / total\n",
        "    return running_loss / len(data_loader), accuracy\n",
        "\n",
        "# Train the model\n",
        "train(model, train_loader, val_loader, criterion, optimizer, epochs=epochs)\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_accuracy = evaluate(model, test_loader, criterion)\n",
        "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
