{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5A5tEQlo_qU"
      },
      "outputs": [],
      "source": [
        "# !pip install wandb -qU\n",
        "# import wandb\n",
        "# wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zXcLBaj8o_qb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5189b4e-4633-442e-fb0d-3d887ca7ce74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvjitlink_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (19.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.1.105 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from torchsummary import summary\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "from torchvision.datasets.utils import download_url\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaNst3uBBmrM"
      },
      "source": [
        "# Import Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('./download'):\n",
        "  # Add your donwload path here\n",
        "  dataset_url = 'https://drive.google.com/file/d/14swfL7Hz3FbX_DweHYkA8XWmucc8lf0M/view?usp=sharing'\n",
        "  download_url(dataset_url,'./download', filename='generated_images_10Kids_cropped.zip')\n",
        "\n",
        "  with zipfile.ZipFile('./download/generated_images_10Kids_cropped.zip', 'r') as tar:\n",
        "      tar.extractall(path='.')\n",
        "\n",
        "  image_folder = './generated_images_10Kids_cropped'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kVjaxHovz2a",
        "outputId": "e9288131-2efe-4e17-d2b6-e3df03194364"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=14swfL7Hz3FbX_DweHYkA8XWmucc8lf0M\n",
            "From (redirected): https://drive.usercontent.google.com/download?id=14swfL7Hz3FbX_DweHYkA8XWmucc8lf0M&confirm=t&uuid=aaabf188-8c9e-416f-98fe-b43588b21eba\n",
            "To: /content/download/generated_images_10Kids_cropped.zip\n",
            "100%|██████████| 4.02G/4.02G [01:24<00:00, 47.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTqCbxGBEW88"
      },
      "source": [
        "# DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JnF1_OZzEUDO"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class FaceDataset(Dataset):\n",
        "    def __init__(self, image_folder, people_dirs, transform=None):\n",
        "        self.image_folder = image_folder\n",
        "        self.transform = transform\n",
        "        self.people_dirs = people_dirs\n",
        "        self.image_pairs = []\n",
        "        self.labels = []\n",
        "        self._prepare_data()\n",
        "\n",
        "    def _prepare_data(self):\n",
        "        #for person_dir in people_dirs:\n",
        "        for i in range(1, int(len(self.people_dirs))):\n",
        "            person_dir = self.people_dirs[i]\n",
        "            person_path = os.path.join(self.image_folder, person_dir)\n",
        "            images = os.listdir(person_path)\n",
        "            for i in range(len(images)):\n",
        "                for j in range(i + 1, len(images)):\n",
        "                    self.image_pairs.append((os.path.join(person_path, images[i]), os.path.join(person_path, images[j])))\n",
        "                    self.labels.append(1)\n",
        "\n",
        "                    # Add negative samples\n",
        "                    neg_person = person_dir\n",
        "                    while neg_person == person_dir:\n",
        "                        neg_person = self.people_dirs[torch.randint(len(self.people_dirs), (1,)).item()]\n",
        "\n",
        "\n",
        "                    neg_images = os.listdir(os.path.join(self.image_folder, neg_person))\n",
        "                    #if(neg_images):\n",
        "                    self.image_pairs.append((os.path.join(person_path, images[i]), os.path.join(self.image_folder, neg_person, neg_images[0])))\n",
        "                    self.labels.append(0)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img1_path, img2_path = self.image_pairs[idx]\n",
        "        label = self.labels[idx]\n",
        "        img1 = Image.open(img1_path).convert('L')\n",
        "        img2 = Image.open(img2_path).convert('L')\n",
        "\n",
        "        if self.transform:\n",
        "            img1 = self.transform(img1)\n",
        "            img2 = self.transform(img2)\n",
        "\n",
        "        return img1, img2, torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "# Split dataset into training, validation, and test sets\n",
        "def split_dataset(dataset, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
        "    total_len = len(dataset)\n",
        "    indices = list(range(total_len))\n",
        "    random.shuffle(indices)\n",
        "\n",
        "    train_end = int(train_ratio * total_len)\n",
        "    val_end = train_end + int(val_ratio * total_len)\n",
        "\n",
        "    train_indices = indices[:train_end]\n",
        "    val_indices = indices[train_end:val_end]\n",
        "    test_indices = indices[val_end:]\n",
        "\n",
        "    return train_indices, val_indices, test_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S2GhNHiBpMk"
      },
      "source": [
        "# Load datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGmY-kjMBlGn",
        "outputId": "5a7b0442-2dc8-4623-c403-946df34b0f60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size: 128\n",
            "LR: 0.01\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters and setup\n",
        "batch_size = 128\n",
        "learning_rate = 0.01\n",
        "#epochs = 1\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f'Batch size: {batch_size}')\n",
        "print(f'LR: {learning_rate}')\n",
        "#print(f'Epochs: {epochs}')\n",
        "print(f'Device: {device}')\n",
        "\n",
        "\n",
        "# Data augmentation and normalization\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((112, 112)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Split before\n",
        "people_dirs = os.listdir(image_folder)\n",
        "\n",
        "num_people = len(people_dirs)\n",
        "\n",
        "# Calculate the number of people to use (40%)\n",
        "num_to_use = int(num_people * 0.4)\n",
        "\n",
        "# Randomly select 40% of the people\n",
        "selected_people = random.sample(people_dirs, num_to_use)\n",
        "\n",
        "\n",
        "# train-val and test set\n",
        "train_val_dirs, test_dirs = train_test_split(selected_people, test_size=0.2, random_state=42)\n",
        "# train and val set\n",
        "train_dirs, val_dirs  = train_test_split(train_val_dirs, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "train_dataset = FaceDataset(image_folder, train_dirs, transform=transform)\n",
        "val_dataset = FaceDataset(image_folder, val_dirs, transform=transform)\n",
        "test_dataset = FaceDataset(image_folder, test_dirs, transform=transform)\n",
        "#train_indices, val_indices, test_indices = split_dataset(dataset)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "#train_loader = DataLoader(dataset, batch_size=batch_size, sampler=torch.utils.data.SubsetRandomSampler(train_indices))\n",
        "#val_loader = DataLoader(dataset, batch_size=batch_size, sampler=torch.utils.data.SubsetRandomSampler(val_indices))\n",
        "#test_loader = DataLoader(dataset, batch_size=batch_size, sampler=torch.utils.data.SubsetRandomSampler(test_indices))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.model_selection import train_test_split\n",
        "# Split before\n",
        "#people_dirs = os.listdir(image_folder)\n",
        "#train_people, val_people = train_test_split(people_dirs, test_size=0.2, random_state=42)\n",
        "#print(len(train_people), len(val_people))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_pVCF8whflc",
        "outputId": "fe350eb2-5ca5-4600-f002-b1a9a6ba2c8c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8000 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeyxydvxEh4r"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gkcPuNcFo_qc"
      },
      "outputs": [],
      "source": [
        "# Siamese Network\n",
        "class TinySiameseNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TinySiameseNetwork, self).__init__()\n",
        "        self.conv_net = nn.Sequential(\n",
        "            nn.Conv2d(1, 2, kernel_size=3, stride=1, padding=1), # double to (1,4...)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(2, 4, kernel_size=3, stride=1, padding=1), # double to (4,8...)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(4, 8, kernel_size=3, stride=1, padding=1), # double to (4,8...)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(8, 4, kernel_size=3, stride=1, padding=1), # double to (4,8...)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(4, 2, kernel_size=3, stride=1, padding=1), # double to (4,8...)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(2*28*28, 8), # double to (8*28*28, 16)\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(8, 1), # double to (16,1)\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward_once(self, x):\n",
        "        output = self.conv_net(x)\n",
        "        output = output.view(output.size()[0], -1)\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        output1 = self.forward_once(img1)\n",
        "        output2 = self.forward_once(img2)\n",
        "        return torch.abs(output1 - output2)\n",
        "\n",
        "\n",
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(32 * 14 * 14, 32)  # assuming the final conv layer's output is 14x14\n",
        "        self.fc2 = nn.Linear(32, 16)\n",
        "        self.fc3 = nn.Linear(16, 1)\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)  # output size: (8, 56, 56)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)  # output size: (16, 28, 28)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.max_pool2d(x, 2)  # output size: (32, 14, 14)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        output1 = self.forward_one(input1)\n",
        "        output2 = self.forward_one(input2)\n",
        "        distance = torch.abs(output1 - output2)\n",
        "        output = torch.sigmoid(self.fc3(distance))\n",
        "        return output\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJId27TtEpgN"
      },
      "source": [
        "# Train and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-AEqf8SPEtZ6"
      },
      "outputs": [],
      "source": [
        "# Evaluation function\n",
        "def evaluate(model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for img1, img2, label in data_loader:\n",
        "            img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
        "            outputs = model(img1, img2).squeeze()\n",
        "            loss = criterion(outputs, label)\n",
        "            running_loss += loss.item()\n",
        "            predicted = (outputs > 0.5).float()\n",
        "            correct += (predicted == label).sum().item()\n",
        "            total += label.size(0)\n",
        "    accuracy = correct / total\n",
        "    return running_loss / len(data_loader), accuracy\n",
        "\n",
        "# Training script with validation\n",
        "def train(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\") as pbar:\n",
        "            for img1, img2, label in train_loader:\n",
        "                img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(img1, img2).squeeze()\n",
        "                loss = criterion(outputs, label)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "                pbar.set_postfix(loss=running_loss / (pbar.n + 1))\n",
        "                pbar.update(1)\n",
        "\n",
        "        val_loss, val_accuracy = evaluate(model, val_loader, criterion)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {running_loss/len(train_loader)}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}\")\n",
        "\n",
        "        # Save the model\n",
        "        torch.save(model.state_dict(), f'./network_epoch{epochs}.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqe8wphDEuiA"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1\n",
        "\n",
        "# Model, loss, and optimizer\n",
        "model = SiameseNetwork().to(device)\n",
        "summary(model, [(1, 112, 112), (1, 112, 112)])\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "train(model, train_loader, val_loader, criterion, optimizer, epochs=epochs)\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_accuracy = evaluate(model, test_loader, criterion)\n",
        "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "USJc1niw-RnJ",
        "outputId": "d4869581-b87e-4c40-c57c-dcb49353dfcd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 8, 112, 112]              80\n",
            "            Conv2d-2           [-1, 16, 56, 56]           1,168\n",
            "            Conv2d-3           [-1, 32, 28, 28]           4,640\n",
            "            Linear-4                   [-1, 32]         200,736\n",
            "            Linear-5                   [-1, 16]             528\n",
            "            Conv2d-6          [-1, 8, 112, 112]              80\n",
            "            Conv2d-7           [-1, 16, 56, 56]           1,168\n",
            "            Conv2d-8           [-1, 32, 28, 28]           4,640\n",
            "            Linear-9                   [-1, 32]         200,736\n",
            "           Linear-10                   [-1, 16]             528\n",
            "           Linear-11                    [-1, 1]              17\n",
            "================================================================\n",
            "Total params: 414,321\n",
            "Trainable params: 414,321\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 600.25\n",
            "Forward/backward pass size (MB): 2.68\n",
            "Params size (MB): 1.58\n",
            "Estimated Total Size (MB): 604.51\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1: 100%|██████████| 7597/7597 [32:20<00:00,  3.92batch/s, loss=0.441]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Train Loss: 0.44087963281092685, Val Loss: 0.3938142698654636, Val Accuracy: 0.8249073387694589\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Parent directory networks does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-89721e835113>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Evaluate on test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-ecc198a33fad>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Save the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'networks/network_epoch{epoch}.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m             \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_disable_byteorder_record\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_zipfile_writer_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Parent directory networks does not exist."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = evaluate(model, test_loader, criterion)"
      ],
      "metadata": {
        "id": "syxCPjI84FXX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Em-GTRl24WDr",
        "outputId": "5987dc7d-772d-4c94-fbeb-81e4e4f4b14d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.393432137003389, Test Accuracy: 0.8247776826296028\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}