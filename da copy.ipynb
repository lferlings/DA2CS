{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "b5A5tEQlo_qU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.1.1 -> 24.1.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtnartsch\u001b[0m (\u001b[33mda2cs\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install wandb -qU\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-RqFIM-o_qb",
        "outputId": "97540303-32b8-468d-8d88-7b8c2873526f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: torch in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (2.5.0.dev20240702+cu124)\n",
            "Requirement already satisfied: torchvision in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (0.20.0.dev20240703+cu124)\n",
            "Requirement already satisfied: torchaudio in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (2.4.0.dev20240703+cu124)\n",
            "Requirement already satisfied: filelock in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (from torchvision) (9.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: tqdm in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (4.66.4)\n",
            "Requirement already satisfied: torchsummary in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (1.5.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.1.1 -> 24.1.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm torchsummary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zXcLBaj8o_qb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ioKil3xi95PM"
      },
      "outputs": [],
      "source": [
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(8)\n",
        "        self.conv2 = nn.Conv2d(8, 16, kernel_size=5, padding=0)\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=0)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(32 * 12 * 12, 41)  # Updated to 32 * 12 * 12\n",
        "        self.fc1komma5 = nn.Linear(41,32)\n",
        "        self.fc2 = nn.Linear(32, 16)\n",
        "        self.fc3 = nn.Linear(16, 1)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        x = F.relu(self.conv1(x)) # 8 * 112 * 112\n",
        "        x = F.max_pool2d(x, 2)  # output size: (8, 56, 56)\n",
        "        x = F.relu(self.conv2(x)) # 16* 52 * 52\n",
        "        x = F.max_pool2d(x, 2)  # output size: (16, 26, 26)\n",
        "        x = F.relu(self.conv3(x)) # 32 * 24 * 24\n",
        "        x = F.max_pool2d(x, 2)  # output size: (32, 12, 12)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc1komma5(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        output1 = self.forward_one(input1)\n",
        "        output2 = self.forward_one(input2)\n",
        "        distance = torch.abs(output1 - output2)\n",
        "        output = self.fc3(distance)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bvxdb-lgo_qc",
        "outputId": "78171c77-1c04-4e9a-c2b9-ebe6f268635f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:sxk53c82) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▄▅▇█</td></tr><tr><td>train_loss</td><td>█▃▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▇▇▇██</td></tr><tr><td>val_loss</td><td>█▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>6</td></tr><tr><td>train_loss</td><td>0.42408</td></tr><tr><td>val_accuracy</td><td>0.80559</td></tr><tr><td>val_loss</td><td>0.42701</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">cosmic-music-7</strong> at: <a href='https://wandb.ai/da2cs/face-recognition-philip/runs/sxk53c82' target=\"_blank\">https://wandb.ai/da2cs/face-recognition-philip/runs/sxk53c82</a><br/> View project at: <a href='https://wandb.ai/da2cs/face-recognition-philip' target=\"_blank\">https://wandb.ai/da2cs/face-recognition-philip</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20240707_223849-sxk53c82\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:sxk53c82). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.4"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\phili\\Pythonprojects\\DA2CS\\wandb\\run-20240708_092841-p8q2stz7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da2cs/face-recognition-philip/runs/p8q2stz7' target=\"_blank\">glowing-tree-8</a></strong> to <a href='https://wandb.ai/da2cs/face-recognition-philip' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/da2cs/face-recognition-philip' target=\"_blank\">https://wandb.ai/da2cs/face-recognition-philip</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/da2cs/face-recognition-philip/runs/p8q2stz7' target=\"_blank\">https://wandb.ai/da2cs/face-recognition-philip/runs/p8q2stz7</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size: 512\n",
            "LR: 0.02\n",
            "Epochs: 10\n",
            "Device: cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 8, 112, 112]              80\n",
            "            Conv2d-2           [-1, 16, 52, 52]           3,216\n",
            "            Conv2d-3           [-1, 32, 24, 24]           4,640\n",
            "            Linear-4                   [-1, 41]         188,969\n",
            "            Linear-5                   [-1, 32]           1,344\n",
            "            Linear-6                   [-1, 16]             528\n",
            "            Conv2d-7          [-1, 8, 112, 112]              80\n",
            "            Conv2d-8           [-1, 16, 52, 52]           3,216\n",
            "            Conv2d-9           [-1, 32, 24, 24]           4,640\n",
            "           Linear-10                   [-1, 41]         188,969\n",
            "           Linear-11                   [-1, 32]           1,344\n",
            "           Linear-12                   [-1, 16]             528\n",
            "           Linear-13                    [-1, 1]              17\n",
            "================================================================\n",
            "Total params: 397,571\n",
            "Trainable params: 397,571\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 600.25\n",
            "Forward/backward pass size (MB): 2.47\n",
            "Params size (MB): 1.52\n",
            "Estimated Total Size (MB): 604.24\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10:  68%|██████▊   | 4514/6680 [1:23:42<40:10,  1.11s/batch, loss=0.516]  \n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 170\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_loader), accuracy\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# Evaluate on test set\u001b[39;00m\n\u001b[0;32m    173\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader, criterion)\n",
            "Cell \u001b[1;32mIn[10], line 122\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[0;32m    120\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i,(img1, img2, label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m    123\u001b[0m         img1, img2, label \u001b[38;5;241m=\u001b[39m img1\u001b[38;5;241m.\u001b[39mto(device), img2\u001b[38;5;241m.\u001b[39mto(device), label\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
            "File \u001b[1;32mc:\\Users\\phili\\Pythonprojects\\DA2CS\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:700\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    698\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 700\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    706\u001b[0m ):\n",
            "File \u001b[1;32mc:\\Users\\phili\\Pythonprojects\\DA2CS\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:756\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    754\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    755\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 756\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    758\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32mc:\\Users\\phili\\Pythonprojects\\DA2CS\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32mc:\\Users\\phili\\Pythonprojects\\DA2CS\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "Cell \u001b[1;32mIn[10], line 40\u001b[0m, in \u001b[0;36mFaceDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     38\u001b[0m img1_path, img2_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_pairs[idx]\n\u001b[0;32m     39\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[1;32m---> 40\u001b[0m img1 \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg1_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mL\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m img2 \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img2_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
            "File \u001b[1;32mc:\\Users\\phili\\Pythonprojects\\DA2CS\\.venv\\lib\\site-packages\\PIL\\Image.py:921\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[0;32m    875\u001b[0m ):\n\u001b[0;32m    876\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 921\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    923\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    924\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    925\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\phili\\Pythonprojects\\DA2CS\\.venv\\lib\\site-packages\\PIL\\ImageFile.py:260\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    255\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage file is truncated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    256\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(b)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bytes not processed)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    257\u001b[0m         )\n\u001b[0;32m    259\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 260\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(8)\n",
        "        self.conv2 = nn.Conv2d(8, 16, kernel_size=5, padding=0)\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=0)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(32 * 12 * 12, 41)  # Updated to 32 * 12 * 12\n",
        "        self.fc1komma5 = nn.Linear(41,32)\n",
        "        self.fc2 = nn.Linear(32, 16)\n",
        "        self.fc3 = nn.Linear(16, 1)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        x = F.relu(self.conv1(x)) # 8 * 112 * 112\n",
        "        x = F.max_pool2d(x, 2)  # output size: (8, 56, 56)\n",
        "        x = F.relu(self.conv2(x)) # 16* 52 * 52\n",
        "        x = F.max_pool2d(x, 2)  # output size: (16, 26, 26)\n",
        "        x = F.relu(self.conv3(x)) # 32 * 24 * 24\n",
        "        x = F.max_pool2d(x, 2)  # output size: (32, 12, 12)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc1komma5(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "    def forward(self, input1, input2, input3, input4):\n",
        "        output1 = self.forward_one(input1)\n",
        "        output2 = self.forward_one(input2)\n",
        "        output3 = self.forward_one(input3)\n",
        "        output4 = self.forward_one(input4)\n",
        "        return output1, output2, output3, output4\n",
        "    \n",
        "class QuadrupletLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(QuadrupletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, anchor, positive, negative1, negative2):\n",
        "        distance_pos = F.pairwise_distance(anchor, positive)\n",
        "        distance_neg1 = F.pairwise_distance(anchor, negative1)\n",
        "        distance_neg2 = F.pairwise_distance(positive, negative2)\n",
        "\n",
        "        loss = torch.mean(F.relu(distance_pos - distance_neg1 + self.margin)) + \\\n",
        "               torch.mean(F.relu(distance_pos - distance_neg2 + self.margin))\n",
        "        return loss\n",
        "\n",
        "\n",
        "class FaceDataset(Dataset):\n",
        "    def __init__(self, image_folder, people_dirs, transform=None):\n",
        "        self.image_folder = image_folder\n",
        "        self.people_dirs = people_dirs\n",
        "        self.transform = transform\n",
        "        self.quadruplets = []\n",
        "        self._prepare_data()\n",
        "\n",
        "    def _prepare_data(self):\n",
        "        for person_dir in self.people_dirs:\n",
        "            person_path = os.path.join(self.image_folder, person_dir)\n",
        "            images = os.listdir(person_path)\n",
        "            for i in range(len(images)):\n",
        "                for j in range(i + 1, len(images)):\n",
        "                    # Anchor and Positive pair\n",
        "                    anchor = os.path.join(person_path, images[i])\n",
        "                    positive = os.path.join(person_path, images[j])\n",
        "\n",
        "                    # Negative samples\n",
        "                    neg_person1 = person_dir\n",
        "                    while neg_person1 == person_dir:\n",
        "                        neg_person1 = random.choice(self.people_dirs)\n",
        "                    neg_images1 = os.listdir(os.path.join(self.image_folder, neg_person1))\n",
        "                    negative1 = os.path.join(self.image_folder, neg_person1, random.choice(neg_images1))\n",
        "\n",
        "                    neg_person2 = person_dir\n",
        "                    while neg_person2 == person_dir or neg_person2 == neg_person1:\n",
        "                        neg_person2 = random.choice(self.people_dirs)\n",
        "                    neg_images2 = os.listdir(os.path.join(self.image_folder, neg_person2))\n",
        "                    negative2 = os.path.join(self.image_folder, neg_person2, random.choice(neg_images2))\n",
        "\n",
        "                    self.quadruplets.append((anchor, positive, negative1, negative2))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.quadruplets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        anchor_path, positive_path, negative1_path, negative2_path = self.quadruplets[idx]\n",
        "        anchor = Image.open(anchor_path).convert('L')\n",
        "        positive = Image.open(positive_path).convert('L')\n",
        "        negative1 = Image.open(negative1_path).convert('L')\n",
        "        negative2 = Image.open(negative2_path).convert('L')\n",
        "\n",
        "        if self.transform:\n",
        "            anchor = self.transform(anchor)\n",
        "            positive = self.transform(positive)\n",
        "            negative1 = self.transform(negative1)\n",
        "            negative2 = self.transform(negative2)\n",
        "\n",
        "        return anchor, positive, negative1, negative2\n",
        "\n",
        "\n",
        "# Function to split dataset\n",
        "def split_dataset(image_folder, train_ratio=0.9, val_ratio=0.05, test_ratio=0.05):\n",
        "    people_dirs = os.listdir(image_folder)\n",
        "    random.shuffle(people_dirs)\n",
        "\n",
        "    train_end = int(train_ratio * len(people_dirs))\n",
        "    val_end = train_end + int(val_ratio * len(people_dirs))\n",
        "\n",
        "    train_dirs = people_dirs[:train_end]\n",
        "    val_dirs = people_dirs[train_end:val_end]\n",
        "    test_dirs = people_dirs[val_end:]\n",
        "\n",
        "    return train_dirs, val_dirs, test_dirs\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(project='face-recognition-philip')\n",
        "\n",
        "# Hyperparameters and setup\n",
        "batch_size = 512\n",
        "learning_rate = 0.02\n",
        "epochs = 10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "wandb.config.update({\n",
        "    \"batch_size\": batch_size,\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"epochs\": epochs,\n",
        "    \"device\": str(device)\n",
        "})\n",
        "\n",
        "print(f'Batch size: {batch_size}')\n",
        "print(f'LR: {learning_rate}')\n",
        "print(f'Epochs: {epochs}')\n",
        "print(f'Device: {device}')\n",
        "\n",
        "# Data augmentation and normalization\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((112, 112)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Load dataset\n",
        "image_folder = 'generated_images_10Kids_cropped'  # Update with the path to your dataset\n",
        "train_dirs, val_dirs, test_dirs = split_dataset(image_folder)\n",
        "\n",
        "train_dataset = FaceDataset(image_folder, train_dirs, transform=transform)\n",
        "val_dataset = FaceDataset(image_folder, val_dirs, transform=transform)\n",
        "test_dataset = FaceDataset(image_folder, test_dirs, transform=transform)\n",
        "# Use multiprocessing to load data\n",
        "\n",
        "# Load data using multiprocessing\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Model, loss, and optimizer\n",
        "model = SiameseNetwork().to(device)\n",
        "summary(model, [(1, 112, 112), (1, 112, 112)])\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
        "scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "# Training script with validation\n",
        "def train(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
        "    accumulation_steps = 4\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\") as pbar:\n",
        "            for i,(img1, img2, label) in enumerate(train_loader):\n",
        "                img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
        "                with torch.amp.autocast('cuda'):\n",
        "                    outputs = model(img1, img2).squeeze()\n",
        "                    loss = criterion(outputs, label)\n",
        "                    loss = loss / accumulation_steps\n",
        "                scaler.scale(loss).backward()\n",
        "                if (i+1) % accumulation_steps == 0:\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                    optimizer.zero_grad()\n",
        "                running_loss += loss.item() * accumulation_steps\n",
        "                pbar.set_postfix(loss=running_loss / (pbar.n + 1))\n",
        "                pbar.update(1)\n",
        "        scheduler.step()\n",
        "        val_loss, val_accuracy = evaluate(model, val_loader, criterion)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {running_loss/len(train_loader)}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}\")\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": running_loss / len(train_loader),\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_accuracy\": val_accuracy\n",
        "        })\n",
        "\n",
        "        # Save the model\n",
        "        torch.save(model.state_dict(), f'networks/network_epoch{epoch}.pth')\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for img1, img2, img3, img4 in data_loader:\n",
        "            img1, img2, img3, img4 = img1.to(device), img2.to(device), img3.to(device), img4.to(device)\n",
        "            with torch.cuda.amp.autocast():\n",
        "                output1, output2, output3, output4 = model(img1, img2, img3, img4)\n",
        "                loss = criterion(output1, output2, output3, output4)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy (you can adjust this based on your task)\n",
        "            distance_pos = F.pairwise_distance(output1, output2)\n",
        "            distance_neg = F.pairwise_distance(output3, output4)\n",
        "            predicted = (distance_pos < distance_neg).float()  # Adjust as per your task\n",
        "            # Assuming label is 1 for positive pair and 0 for negative pair\n",
        "            correct += (predicted == 1).sum().item()\n",
        "            total += img1.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return running_loss / len(data_loader), accuracy\n",
        "\n",
        "# Train the model\n",
        "train(model, train_loader, val_loader, criterion, optimizer, epochs=epochs)\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_accuracy = evaluate(model, test_loader, criterion)\n",
        "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n",
        "\n",
        "# Log final test metrics to wandb\n",
        "wandb.log({\n",
        "    \"test_loss\": test_loss,\n",
        "    \"test_accuracy\": test_accuracy\n",
        "})\n",
        "\n",
        "# Finish wandb run\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "Number of GPUs: 1\n",
            "GPU Name: NVIDIA GeForce RTX 3060\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Image file not found for pair 5608. Returning None.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\phili\\AppData\\Local\\Temp\\ipykernel_10676\\2900512154.py:64: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('networks/final_network.pth'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mated Scores: [0.9458690285682678, 0.9680026173591614, 0.05446825921535492, 0.5319024324417114, 0.6844567656517029]\n",
            "Non-Mated Scores: [0.07858853042125702, 6.347186717903242e-05, 0.24736875295639038, 8.3736922533717e-05, 0.9689295291900635]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "class RealTestDataset(Dataset):\n",
        "    def __init__(self, image_folder, pairs_file, transform=None):\n",
        "        self.image_folder = image_folder\n",
        "        self.pairs = self._read_pairs(pairs_file)\n",
        "        self.transform = transform\n",
        "\n",
        "    def _read_pairs(self, pairs_file):\n",
        "        pairs = []\n",
        "        with open(pairs_file, 'r') as file:\n",
        "            for line in file:\n",
        "                name_1, number_1, name_2, number_2 = line.strip().split()\n",
        "                img1_path = os.path.join(self.image_folder, name_1, f'{name_1}_{int(number_1):04d}.png')\n",
        "                img2_path = os.path.join(self.image_folder, name_2, f'{name_2}_{int(number_2):04d}.png')\n",
        "                label = 1 if name_1 == name_2 else 0\n",
        "                pairs.append((img1_path.replace('\\\\', '/'), img2_path.replace('\\\\', '/'), label))\n",
        "        return pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img1_path, img2_path, label = self.pairs[idx]\n",
        "        try:\n",
        "            img1 = Image.open(img1_path).convert('L')\n",
        "            img2 = Image.open(img2_path).convert('L')\n",
        "        except FileNotFoundError:\n",
        "            # Handle missing image files\n",
        "            print(f\"Warning: Image file not found for pair {idx}. Returning None.\")\n",
        "            return None\n",
        "\n",
        "        if self.transform:\n",
        "            img1 = self.transform(img1) if img1 else None\n",
        "            img2 = self.transform(img2) if img2 else None\n",
        "\n",
        "        return img1, img2, torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "# Define the transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((112, 112)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Initialize the dataset and dataloader\n",
        "image_folder = 'lfw_cropped/lfw_cropped'  # Update with the path to your unzipped dataset folder\n",
        "pairs_file = 'pairs.txt'  # Update with the path to pairs.txt\n",
        "test_dataset = RealTestDataset(image_folder, pairs_file, transform=transform)\n",
        "\n",
        "# Filter out pairs with missing images and create DataLoader\n",
        "filtered_pairs = [pair for pair in test_dataset if pair is not None]\n",
        "filtered_dataset = [x for x in filtered_pairs if x is not None]\n",
        "test_loader = DataLoader(filtered_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Load the saved model weights\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SiameseNetwork().to(device)\n",
        "model.load_state_dict(torch.load('networks/final_network.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, data_loader):\n",
        "    model.eval()\n",
        "    mated_scores = []\n",
        "    non_mated_scores = []\n",
        "    with torch.no_grad():\n",
        "        for img1, img2, label in data_loader:\n",
        "            if img1 is None or img2 is None:\n",
        "                # Handle missing images: Skip or assign a placeholder score\n",
        "                continue\n",
        "            \n",
        "            img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
        "            outputs = model(img1, img2).squeeze()\n",
        "            score = torch.sigmoid(outputs).item()\n",
        "            if label.item() == 1:\n",
        "                mated_scores.append(score)\n",
        "            else:\n",
        "                non_mated_scores.append(score)\n",
        "    \n",
        "    # Pad the shorter list with -1 for uneven lengths (optional)\n",
        "    max_len = max(len(mated_scores), len(non_mated_scores))\n",
        "    mated_scores.extend([-1] * (max_len - len(mated_scores)))\n",
        "    non_mated_scores.extend([-1] * (max_len - len(non_mated_scores)))\n",
        "    \n",
        "    return mated_scores, non_mated_scores\n",
        "\n",
        "# Evaluate on the real test set\n",
        "mated_scores, non_mated_scores = evaluate(model, test_loader)\n",
        "\n",
        "# Save mated and non-mated scores to .txt files\n",
        "mated_scores_file = 'mated_scores.txt'\n",
        "non_mated_scores_file = 'non_mated_scores.txt'\n",
        "\n",
        "with open(mated_scores_file, 'w') as f:\n",
        "    for score in mated_scores:\n",
        "        f.write(f'{score}\\n')\n",
        "\n",
        "with open(non_mated_scores_file, 'w') as f:\n",
        "    for score in non_mated_scores:\n",
        "        f.write(f'{score}\\n')\n",
        "\n",
        "print(f'Mated Scores: {mated_scores[:5]}')\n",
        "print(f'Non-Mated Scores: {non_mated_scores[:5]}')\n",
        "\n",
        "# Zip the files\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('predictions.zip', 'w') as zipf:\n",
        "    zipf.write(mated_scores_file)\n",
        "    zipf.write(non_mated_scores_file)\n",
        "\n",
        "# Optionally, save the results to CSV\n",
        "results_df = pd.DataFrame({'Mated_Scores': mated_scores, 'Non_Mated_Scores': non_mated_scores})\n",
        "results_df.to_csv('evaluation_results.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\phili\\AppData\\Local\\Temp\\ipykernel_10676\\1887187378.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('networks/final_network.pth'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.5614\n",
            "Precision: 1.0000\n",
            "Recall: 0.5614\n",
            "F1-score: 0.7191\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "class CustomTestDataset(Dataset):\n",
        "    def __init__(self, image_folder, transform=None):\n",
        "        self.image_folder = image_folder\n",
        "        self.transform = transform\n",
        "        self.subfolders = [subfolder for subfolder in os.listdir(image_folder) if os.path.isdir(os.path.join(image_folder, subfolder))]\n",
        "        self.pairs = self._create_pairs()\n",
        "\n",
        "    def _create_pairs(self):\n",
        "        pairs = []\n",
        "        for subfolder in self.subfolders:\n",
        "            subfolder_path = os.path.join(self.image_folder, subfolder)\n",
        "            images = [img for img in os.listdir(subfolder_path) if img.endswith('.png')]\n",
        "            if len(images) < 2:\n",
        "                continue  # Skip folders with fewer than 2 images\n",
        "            for i in range(len(images)):\n",
        "                for j in range(i+1, len(images)):\n",
        "                    img1_path = os.path.join(subfolder_path, images[i])\n",
        "                    img2_path = os.path.join(subfolder_path, images[j])\n",
        "                    label = 1 if images[i].split('_')[0] == images[j].split('_')[0] else 0\n",
        "                    pairs.append((img1_path.replace('\\\\', '/'), img2_path.replace('\\\\', '/'), label))\n",
        "        return pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img1_path, img2_path, label = self.pairs[idx]\n",
        "        try:\n",
        "            img1 = Image.open(img1_path).convert('L')\n",
        "            img2 = Image.open(img2_path).convert('L')\n",
        "        except FileNotFoundError:\n",
        "            # Handle missing image files\n",
        "            print(f\"Warning: Image file not found for pair {idx}. Returning None.\")\n",
        "            return None\n",
        "\n",
        "        if self.transform:\n",
        "            img1 = self.transform(img1) if img1 else None\n",
        "            img2 = self.transform(img2) if img2 else None\n",
        "\n",
        "        return img1, img2, torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "# Define the transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((112, 112)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Initialize the dataset and dataloader\n",
        "image_folder = 'lfw_cropped/lfw_cropped'  # Update with the path to your unzipped dataset folder\n",
        "test_dataset = CustomTestDataset(image_folder, transform=transform)\n",
        "\n",
        "# Create DataLoader for the entire dataset\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "\n",
        "# Load the saved model weights\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SiameseNetwork().to(device)\n",
        "model.load_state_dict(torch.load('networks/final_network.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, data_loader):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for img1, img2, label in data_loader:\n",
        "            if img1 is None or img2 is None:\n",
        "                # Handle missing images: Skip or assign a placeholder prediction\n",
        "                all_labels.append(label.item())\n",
        "                all_predictions.append(-1)  # Placeholder for missing image\n",
        "                continue\n",
        "            \n",
        "            img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
        "            outputs = model(img1, img2).squeeze()\n",
        "            score = torch.sigmoid(outputs).item()\n",
        "            prediction = 1 if score >= 0.5 else 0\n",
        "            \n",
        "            all_labels.append(label.item())\n",
        "            all_predictions.append(prediction)\n",
        "    \n",
        "    return all_labels, all_predictions\n",
        "\n",
        "# Evaluate on the entire dataset\n",
        "all_labels, all_predictions = evaluate(model, test_loader)\n",
        "\n",
        "# Compute accuracy, precision, recall, and F1-score\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='binary')\n",
        "\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1-score: {f1:.4f}')\n",
        "\n",
        "# Optionally, save the results to CSV\n",
        "results_df = pd.DataFrame({'Labels': all_labels, 'Predictions': all_predictions})\n",
        "results_df.to_csv('evaluation_results.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(8)\n",
        "        self.conv2 = nn.Conv2d(8, 16, kernel_size=5, padding=0)\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=0)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(32 * 12 * 12, 41)  # Updated to 32 * 12 * 12\n",
        "        self.fc1komma5 = nn.Linear(41,32)\n",
        "        self.fc2 = nn.Linear(32, 16)\n",
        "        self.fc3 = nn.Linear(16, 1)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        x = F.relu(self.conv1(x)) # 8 * 112 * 112\n",
        "        x = F.max_pool2d(x, 2)  # output size: (8, 56, 56)\n",
        "        x = F.relu(self.conv2(x)) # 16* 52 * 52\n",
        "        x = F.max_pool2d(x, 2)  # output size: (16, 26, 26)\n",
        "        x = F.relu(self.conv3(x)) # 32 * 24 * 24\n",
        "        x = F.max_pool2d(x, 2)  # output size: (32, 12, 12)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc1komma5(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "    def forward(self, input1, input2, input3, input4):\n",
        "        output1 = self.forward_one(input1)\n",
        "        output2 = self.forward_one(input2)\n",
        "        output3 = self.forward_one(input3)\n",
        "        output4 = self.forward_one(input4)\n",
        "        return output1, output2, output3, output4\n",
        "    \n",
        "class QuadrupletLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(QuadrupletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, anchor, positive, negative1, negative2):\n",
        "        distance_pos = F.pairwise_distance(anchor, positive)\n",
        "        distance_neg1 = F.pairwise_distance(anchor, negative1)\n",
        "        distance_neg2 = F.pairwise_distance(positive, negative2)\n",
        "\n",
        "        loss = torch.mean(F.relu(distance_pos - distance_neg1 + self.margin)) + \\\n",
        "               torch.mean(F.relu(distance_pos - distance_neg2 + self.margin))\n",
        "        return loss\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
