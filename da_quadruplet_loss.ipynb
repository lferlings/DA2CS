{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install tqdm torchsummary\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from torchsummary import summary\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "from torchvision.datasets.utils import download_url\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ED0QcceGr550",
        "outputId": "af0fd29b-217c-4731-afc5-efa711ecb114"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvjitlink_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (19.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.1.105 nvidia-nvtx-cu12-12.1.105\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "b5A5tEQlo_qU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "8f84b3e1-d348-4eed-cdf6-245fb81299e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.6/300.6 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "!pip install wandb -qU\n",
        "import wandb\n",
        "wandb.login()\n",
        "# 29eae2ad29417280bdd72cb7ee86bc339759dd9c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PG3-_ByI0xF9"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "# # Unzip data\n",
        "#!unzip /content/drive/MyDrive/generated_images_10Kids_cropped.zip -d my_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('./download'):\n",
        "  # Add your donwload path here\n",
        "  dataset_url = 'https://drive.google.com/file/d/14swfL7Hz3FbX_DweHYkA8XWmucc8lf0M/view?usp=sharing'\n",
        "  download_url(dataset_url,'./download', filename='generated_images_10Kids_cropped.zip')\n",
        "\n",
        "  with zipfile.ZipFile('./download/generated_images_10Kids_cropped.zip', 'r') as tar:\n",
        "      tar.extractall(path='.')\n",
        "\n",
        "  image_folder = './generated_images_10Kids_cropped'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-JoMGX_r-KK",
        "outputId": "ae7e77c7-8f2c-466f-87e0-0150a154e537"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=14swfL7Hz3FbX_DweHYkA8XWmucc8lf0M\n",
            "From (redirected): https://drive.usercontent.google.com/download?id=14swfL7Hz3FbX_DweHYkA8XWmucc8lf0M&confirm=t&uuid=b4f26fa5-02af-41d8-bc3b-2e0a43bdd8d5\n",
            "To: /content/download/generated_images_10Kids_cropped.zip\n",
            "100%|██████████| 4.02G/4.02G [00:56<00:00, 70.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ioKil3xi95PM"
      },
      "outputs": [],
      "source": [
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(8)\n",
        "        self.conv2 = nn.Conv2d(8, 16, kernel_size=5, padding=0)\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=0)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(32 * 12 * 12, 41)  # Updated to 32 * 12 * 12\n",
        "        self.fc1komma5 = nn.Linear(41,32)\n",
        "        self.fc2 = nn.Linear(32, 16)\n",
        "        self.fc3 = nn.Linear(16, 1)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        x = F.relu(self.conv1(x)) # 8 * 112 * 112\n",
        "        x = F.max_pool2d(x, 2)  # output size: (8, 56, 56)\n",
        "        x = F.relu(self.conv2(x)) # 16* 52 * 52\n",
        "        x = F.max_pool2d(x, 2)  # output size: (16, 26, 26)\n",
        "        x = F.relu(self.conv3(x)) # 32 * 24 * 24\n",
        "        x = F.max_pool2d(x, 2)  # output size: (32, 12, 12)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc1komma5(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "    def forward(self, input1, input2, input3, input4):\n",
        "        output1 = self.forward_one(input1)\n",
        "        output2 = self.forward_one(input2)\n",
        "        output3 = self.forward_one(input3)\n",
        "        output4 = self.forward_one(input4)\n",
        "        return output1, output2, output3, output4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class QuadrupletLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(QuadrupletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, anchor, positive, negative1, negative2):\n",
        "        distance_pos = F.pairwise_distance(anchor, positive)\n",
        "        distance_neg1 = F.pairwise_distance(anchor, negative1)\n",
        "        distance_neg2 = F.pairwise_distance(positive, negative2)\n",
        "\n",
        "        loss = torch.mean(F.relu(distance_pos - distance_neg1 + self.margin)) + \\\n",
        "               torch.mean(F.relu(distance_pos - distance_neg2 + self.margin))\n",
        "        return loss"
      ],
      "metadata": {
        "id": "g4QdYCTKtPp7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        },
        "id": "Bvxdb-lgo_qc",
        "outputId": "67cdd715-eb64-4ac0-bba7-f8206df0fcaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchrisknaden\u001b[0m (\u001b[33mda2cs\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240708_201639-dasru6js</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da2cs/face-recognition-project/runs/dasru6js' target=\"_blank\">cerulean-bee-36</a></strong> to <a href='https://wandb.ai/da2cs/face-recognition-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da2cs/face-recognition-project' target=\"_blank\">https://wandb.ai/da2cs/face-recognition-project</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da2cs/face-recognition-project/runs/dasru6js' target=\"_blank\">https://wandb.ai/da2cs/face-recognition-project/runs/dasru6js</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size: 512\n",
            "LR: 0.02\n",
            "Epochs: 5\n",
            "Device: cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 8, 112, 112]              80\n",
            "            Conv2d-2           [-1, 16, 52, 52]           3,216\n",
            "            Conv2d-3           [-1, 32, 24, 24]           4,640\n",
            "            Linear-4                   [-1, 41]         188,969\n",
            "            Linear-5                   [-1, 32]           1,344\n",
            "            Linear-6                   [-1, 16]             528\n",
            "            Conv2d-7          [-1, 8, 112, 112]              80\n",
            "            Conv2d-8           [-1, 16, 52, 52]           3,216\n",
            "            Conv2d-9           [-1, 32, 24, 24]           4,640\n",
            "           Linear-10                   [-1, 41]         188,969\n",
            "           Linear-11                   [-1, 32]           1,344\n",
            "           Linear-12                   [-1, 16]             528\n",
            "           Conv2d-13          [-1, 8, 112, 112]              80\n",
            "           Conv2d-14           [-1, 16, 52, 52]           3,216\n",
            "           Conv2d-15           [-1, 32, 24, 24]           4,640\n",
            "           Linear-16                   [-1, 41]         188,969\n",
            "           Linear-17                   [-1, 32]           1,344\n",
            "           Linear-18                   [-1, 16]             528\n",
            "           Conv2d-19          [-1, 8, 112, 112]              80\n",
            "           Conv2d-20           [-1, 16, 52, 52]           3,216\n",
            "           Conv2d-21           [-1, 32, 24, 24]           4,640\n",
            "           Linear-22                   [-1, 41]         188,969\n",
            "           Linear-23                   [-1, 32]           1,344\n",
            "           Linear-24                   [-1, 16]             528\n",
            "================================================================\n",
            "Total params: 795,108\n",
            "Trainable params: 795,108\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 94450499584.00\n",
            "Forward/backward pass size (MB): 4.95\n",
            "Params size (MB): 3.03\n",
            "Estimated Total Size (MB): 94450499591.98\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 2598/2598 [31:20<00:00,  1.38batch/s, loss=0.465]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Train Loss: 0.4649793513698519, Val Loss: 0.3347408224024293, Val Accuracy: 0.9268912280701754\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5:   2%|▏         | 48/2598 [00:35<26:05,  1.63batch/s, loss=0.336]"
          ]
        }
      ],
      "source": [
        "# Define custom FaceDataset\n",
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class FaceDataset(Dataset):\n",
        "    def __init__(self, image_folder, people_dirs, transform=None):\n",
        "        self.image_folder = image_folder\n",
        "        self.people_dirs = people_dirs\n",
        "        self.transform = transform\n",
        "        self.quadruplets = []\n",
        "        self._prepare_data()\n",
        "\n",
        "    def _prepare_data(self):\n",
        "        for person_dir in self.people_dirs:\n",
        "            person_path = os.path.join(self.image_folder, person_dir)\n",
        "            images = os.listdir(person_path)\n",
        "            for i in range(len(images)):\n",
        "                for j in range(i + 1, len(images)):\n",
        "                    # Anchor and Positive pair\n",
        "                    anchor = os.path.join(person_path, images[i])\n",
        "                    positive = os.path.join(person_path, images[j])\n",
        "\n",
        "                    # Negative samples\n",
        "                    neg_person1 = person_dir\n",
        "                    while neg_person1 == person_dir:\n",
        "                        neg_person1 = random.choice(self.people_dirs)\n",
        "                    neg_images1 = os.listdir(os.path.join(self.image_folder, neg_person1))\n",
        "                    negative1 = os.path.join(self.image_folder, neg_person1, random.choice(neg_images1))\n",
        "\n",
        "                    neg_person2 = person_dir\n",
        "                    while neg_person2 == person_dir or neg_person2 == neg_person1:\n",
        "                        neg_person2 = random.choice(self.people_dirs)\n",
        "                    neg_images2 = os.listdir(os.path.join(self.image_folder, neg_person2))\n",
        "                    negative2 = os.path.join(self.image_folder, neg_person2, random.choice(neg_images2))\n",
        "\n",
        "                    self.quadruplets.append((anchor, positive, negative1, negative2))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.quadruplets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        anchor_path, positive_path, negative1_path, negative2_path = self.quadruplets[idx]\n",
        "        anchor = Image.open(anchor_path).convert('L')\n",
        "        positive = Image.open(positive_path).convert('L')\n",
        "        negative1 = Image.open(negative1_path).convert('L')\n",
        "        negative2 = Image.open(negative2_path).convert('L')\n",
        "\n",
        "        if self.transform:\n",
        "            anchor = self.transform(anchor)\n",
        "            positive = self.transform(positive)\n",
        "            negative1 = self.transform(negative1)\n",
        "            negative2 = self.transform(negative2)\n",
        "\n",
        "        return anchor, positive, negative1, negative2\n",
        "\n",
        "\n",
        "# Function to split dataset\n",
        "def split_dataset(image_folder, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
        "    people_dirs = os.listdir(image_folder)\n",
        "    random.shuffle(people_dirs)\n",
        "\n",
        "    train_end = int(train_ratio * len(people_dirs))\n",
        "    val_end = train_end + int(val_ratio * len(people_dirs))\n",
        "\n",
        "    train_dirs = people_dirs[:train_end]\n",
        "    val_dirs = people_dirs[train_end:val_end]\n",
        "    test_dirs = people_dirs[val_end:]\n",
        "\n",
        "    return train_dirs, val_dirs, test_dirs\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(project='face-recognition-project')\n",
        "\n",
        "# Hyperparameters and setup\n",
        "batch_size = 512\n",
        "learning_rate = 0.02\n",
        "epochs = 5\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "wandb.config.update({\n",
        "    \"batch_size\": batch_size,\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"epochs\": epochs,\n",
        "    \"device\": str(device)\n",
        "})\n",
        "\n",
        "print(f'Batch size: {batch_size}')\n",
        "print(f'LR: {learning_rate}')\n",
        "print(f'Epochs: {epochs}')\n",
        "print(f'Device: {device}')\n",
        "\n",
        "# Normal transform\n",
        "transform_normal = transforms.Compose([\n",
        "    transforms.Resize((112, 112)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "# Data augmentation and normalization\n",
        "transform_data_augmentation = transforms.Compose([\n",
        "    transforms.Resize((112, 112)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Load dataset\n",
        "#image_folder = 'generated_images_10Kids_cropped'  # Update with the path to your dataset\n",
        "train_dirs, val_dirs, test_dirs = split_dataset(image_folder)\n",
        "\n",
        "#train_dataset_data_augmentation = FaceDataset(image_folder, train_dirs, transform=transform_data_augmentation)\n",
        "#train_dataset_normal = FaceDataset(image_folder, train_dirs, transform=transform_normal)\n",
        "\n",
        "# train_dataset = ConcatDataset([train_dataset_normal, train_dataset_data_augmentation])\n",
        "train_dataset = FaceDataset(image_folder, train_dirs, transform=transform_data_augmentation)\n",
        "val_dataset = FaceDataset(image_folder, val_dirs, transform=transform_normal)\n",
        "test_dataset = FaceDataset(image_folder, test_dirs, transform=transform_normal)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# Model, loss, and optimizer\n",
        "model = SiameseNetwork().to(device)\n",
        "summary(model, [(1, 112, 112), (1, 112, 112),(1, 112, 112),(1, 112, 112)])\n",
        "#criterion = nn.BCEWithLogitsLoss()\n",
        "criterion = QuadrupletLoss(margin=1.0)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# Training script with validation\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train(model, train_loader, val_loader, criterion, optimizer, scheduler, scaler, epochs):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\") as pbar:\n",
        "            for i, (img1, img2, img3, img4) in enumerate(train_loader):\n",
        "                img1, img2, img3, img4 = img1.to(device), img2.to(device), img3.to(device), img4.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
        "                    # Pass all four inputs to the model\n",
        "                    output1, output2, output3, output4 = model(img1, img2, img3, img4)\n",
        "\n",
        "                    # Calculate loss\n",
        "                    loss = criterion(output1, output2, output3, output4)\n",
        "\n",
        "                if scaler is not None:\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "                pbar.set_postfix(loss=running_loss/(i+1))\n",
        "                pbar.update(1)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation\n",
        "        val_loss, val_accuracy = evaluate(model, val_loader, criterion)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {running_loss / len(train_loader)}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}\")\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": running_loss / len(train_loader),\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_accuracy\": val_accuracy\n",
        "        })\n",
        "\n",
        "        # Save the model\n",
        "        torch.save(model.state_dict(), f'./network_epoch{epoch}.pth')\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for img1, img2, img3, img4 in data_loader:\n",
        "            img1, img2, img3, img4 = img1.to(device), img2.to(device), img3.to(device), img4.to(device)\n",
        "            with torch.cuda.amp.autocast():\n",
        "                output1, output2, output3, output4 = model(img1, img2, img3, img4)\n",
        "                loss = criterion(output1, output2, output3, output4)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy (you can adjust this based on your task)\n",
        "            distance_pos = F.pairwise_distance(output1, output2)\n",
        "            distance_neg = F.pairwise_distance(output3, output4)\n",
        "            predicted = (distance_pos < distance_neg).float()  # Adjust as per your task\n",
        "            # Assuming label is 1 for positive pair and 0 for negative pair\n",
        "            correct += (predicted == 1).sum().item()\n",
        "            total += img1.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return running_loss / len(data_loader), accuracy\n",
        "\n",
        "\n",
        "# Train the model\n",
        "#train(model, train_loader, val_loader, criterion, optimizer, epochs=epochs)\n",
        "train(model, train_loader, val_loader, criterion, optimizer, scheduler, scaler, epochs=epochs)\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_accuracy = evaluate(model, test_loader, criterion)\n",
        "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n",
        "\n",
        "# Log final test metrics to wandb\n",
        "wandb.log({\n",
        "    \"test_loss\": test_loss,\n",
        "    \"test_accuracy\": test_accuracy\n",
        "})\n",
        "\n",
        "# Finish wandb run\n",
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}