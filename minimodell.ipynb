{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "b5A5tEQlo_qU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtnartsch\u001b[0m (\u001b[33mda2cs\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#!pip install wandb -qU\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PG3-_ByI0xF9",
        "outputId": "0c40471f-0ba2-40f6-fd16-267573a63207"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "# # Unzip data\n",
        "#!unzip /content/drive/MyDrive/generated_images_10Kids_cropped.zip -d my_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-RqFIM-o_qb",
        "outputId": "97540303-32b8-468d-8d88-7b8c2873526f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/121, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: torch in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (2.5.0.dev20240702+cu124)\n",
            "Requirement already satisfied: torchvision in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (0.20.0.dev20240703+cu124)\n",
            "Requirement already satisfied: torchaudio in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (2.4.0.dev20240703+cu124)\n",
            "Requirement already satisfied: filelock in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (from torchvision) (9.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/121\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zXcLBaj8o_qb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from torchsummary import summary\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ioKil3xi95PM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        # Convolutional layers with fewer filters\n",
        "        self.conv1 = nn.Conv2d(1, 4, kernel_size=3, padding=1) #Output 112x112\n",
        "        self.conv2 = nn.Conv2d(4, 8, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
        "\n",
        "        # Fully connected layers with fewer units\n",
        "        self.fc1 = nn.Linear(16 * 14 * 14, 16)  # Assuming input image size is 112x112\n",
        "        self.fc2 = nn.Linear(16, 8)\n",
        "        self.fc3 = nn.Linear(8, 1)\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        x = F.relu(self.conv1(x))  # 4 * 112 x 112\n",
        "        x = F.max_pool2d(x, 2)  # output size: (4, 56, 56)\n",
        "        x = F.relu(self.conv2(x))  # 8 * 56 * 56\n",
        "        x = F.max_pool2d(x, 2)  # output size: (8, 28, 28)\n",
        "        x = F.relu(self.conv3(x))  # 16 * 28 * 28\n",
        "        x = F.max_pool2d(x, 2)  # output size: (16, 14, 14)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        output1 = self.forward_one(input1)\n",
        "        output2 = self.forward_one(input2)\n",
        "        distance = torch.abs(output1 - output2)\n",
        "        output = torch.sigmoid(self.fc3(distance))\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bvxdb-lgo_qc",
        "outputId": "78171c77-1c04-4e9a-c2b9-ebe6f268635f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.4"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\phili\\Pythonprojects\\DA2CS\\wandb\\run-20240706_105916-fa1cgvoz</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da2cs/face-dataset-project/runs/fa1cgvoz' target=\"_blank\">olive-smoke-18</a></strong> to <a href='https://wandb.ai/da2cs/face-dataset-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/da2cs/face-dataset-project' target=\"_blank\">https://wandb.ai/da2cs/face-dataset-project</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/da2cs/face-dataset-project/runs/fa1cgvoz' target=\"_blank\">https://wandb.ai/da2cs/face-dataset-project/runs/fa1cgvoz</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size: 128\n",
            "LR: 0.01\n",
            "Epochs: 20\n",
            "Device: cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 4, 112, 112]              40\n",
            "            Conv2d-2            [-1, 8, 56, 56]             296\n",
            "            Conv2d-3           [-1, 16, 28, 28]           1,168\n",
            "            Linear-4                   [-1, 16]          50,192\n",
            "            Linear-5                    [-1, 8]             136\n",
            "            Conv2d-6          [-1, 4, 112, 112]              40\n",
            "            Conv2d-7            [-1, 8, 56, 56]             296\n",
            "            Conv2d-8           [-1, 16, 28, 28]           1,168\n",
            "            Linear-9                   [-1, 16]          50,192\n",
            "           Linear-10                    [-1, 8]             136\n",
            "           Linear-11                    [-1, 1]               9\n",
            "================================================================\n",
            "Total params: 103,673\n",
            "Trainable params: 103,673\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 600.25\n",
            "Forward/backward pass size (MB): 1.34\n",
            "Params size (MB): 0.40\n",
            "Estimated Total Size (MB): 601.99\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20: 100%|██████████| 26718/26718 [1:10:06<00:00,  6.35batch/s, loss=0.473]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20, Train Loss: 0.4729584642538224, Val Loss: 0.4522444335508523, Val Accuracy: 0.7896921052631579\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20: 100%|██████████| 26718/26718 [1:06:16<00:00,  6.72batch/s, loss=0.419]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/20, Train Loss: 0.41895664168698976, Val Loss: 0.40527007150385025, Val Accuracy: 0.8173105263157895\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/20: 100%|██████████| 26718/26718 [1:09:46<00:00,  6.38batch/s, loss=0.398]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/20, Train Loss: 0.3980828533529816, Val Loss: 0.3781250360292913, Val Accuracy: 0.8315789473684211\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/20: 100%|██████████| 26718/26718 [1:06:04<00:00,  6.74batch/s, loss=0.39]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/20, Train Loss: 0.3901404277414911, Val Loss: 0.37874894476511056, Val Accuracy: 0.8318657894736842\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/20: 100%|██████████| 26718/26718 [1:10:06<00:00,  6.35batch/s, loss=0.387]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/20, Train Loss: 0.3869471247608776, Val Loss: 0.37299822161206897, Val Accuracy: 0.8381236842105263\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/20: 100%|██████████| 26718/26718 [1:06:07<00:00,  6.73batch/s, loss=0.385]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/20, Train Loss: 0.38526426914035583, Val Loss: 0.3706727199364327, Val Accuracy: 0.8380394736842105\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/20: 100%|██████████| 26718/26718 [1:06:04<00:00,  6.74batch/s, loss=0.385]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/20, Train Loss: 0.3845244277017953, Val Loss: 0.38022022037122255, Val Accuracy: 0.8303815789473684\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/20: 100%|██████████| 26718/26718 [1:05:56<00:00,  6.75batch/s, loss=0.384]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/20, Train Loss: 0.3839280593364224, Val Loss: 0.3787892572970734, Val Accuracy: 0.8303105263157895\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/20: 100%|██████████| 26718/26718 [1:06:00<00:00,  6.75batch/s, loss=0.383]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/20, Train Loss: 0.3832418143981249, Val Loss: 0.3933728777195878, Val Accuracy: 0.827128947368421\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/20: 100%|██████████| 26718/26718 [1:07:36<00:00,  6.59batch/s, loss=0.383]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/20, Train Loss: 0.3825173033269609, Val Loss: 0.38212710766528907, Val Accuracy: 0.8312394736842105\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/20: 100%|██████████| 26718/26718 [1:10:46<00:00,  6.29batch/s, loss=0.382]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/20, Train Loss: 0.3823622825989619, Val Loss: 0.40613325547744383, Val Accuracy: 0.81485\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/20: 100%|██████████| 26718/26718 [1:08:46<00:00,  6.48batch/s, loss=0.382]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/20, Train Loss: 0.3819678845601405, Val Loss: 0.36727297476202037, Val Accuracy: 0.8390631578947368\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/20: 100%|██████████| 26718/26718 [1:28:34<00:00,  5.03batch/s, loss=0.382]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13/20, Train Loss: 0.3817302140390742, Val Loss: 0.381857728642691, Val Accuracy: 0.8276289473684211\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/20: 100%|██████████| 26718/26718 [1:21:28<00:00,  5.47batch/s, loss=0.382]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "class FaceDataset(Dataset):\n",
        "    def __init__(self, image_folder, people_dirs, transform=None):\n",
        "        self.image_folder = image_folder\n",
        "        self.people_dirs = people_dirs\n",
        "        self.transform = transform\n",
        "        self.image_pairs = []\n",
        "        self.labels = []\n",
        "        self._prepare_data()\n",
        "\n",
        "    def _prepare_data(self):\n",
        "        for person_dir in self.people_dirs:\n",
        "            person_path = os.path.join(self.image_folder, person_dir)\n",
        "            images = os.listdir(person_path)\n",
        "            for i in range(len(images)):\n",
        "                for j in range(i + 1, len(images)):\n",
        "                    self.image_pairs.append((os.path.join(person_path, images[i]), os.path.join(person_path, images[j])))\n",
        "                    self.labels.append(1)\n",
        "\n",
        "                    # Add negative samples\n",
        "                    neg_person = person_dir\n",
        "                    while neg_person == person_dir:\n",
        "                        neg_person = random.choice(self.people_dirs)\n",
        "\n",
        "                    neg_images = os.listdir(os.path.join(self.image_folder, neg_person))\n",
        "                    random_image_index = random.randrange(start=0, stop=len(neg_images))\n",
        "                    self.image_pairs.append((os.path.join(person_path, images[i]), os.path.join(self.image_folder, neg_person, neg_images[random_image_index])))\n",
        "                    self.labels.append(0)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img1_path, img2_path = self.image_pairs[idx]\n",
        "        label = self.labels[idx]\n",
        "        img1 = Image.open(img1_path).convert('L')\n",
        "        img2 = Image.open(img2_path).convert('L')\n",
        "\n",
        "        if self.transform:\n",
        "            img1 = self.transform(img1)\n",
        "            img2 = self.transform(img2)\n",
        "\n",
        "        return img1, img2, torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "# Function to split dataset\n",
        "def split_dataset(image_folder, train_ratio=0.9, val_ratio=0.1, test_ratio=0.0, random_seed=None):\n",
        "    if random_seed is not None: # enable setting a random seed for reproducable splitting\n",
        "        random.seed(random_seed)\n",
        "        torch.manual_seed(random_seed)\n",
        "    people_dirs = os.listdir(image_folder)\n",
        "    random.shuffle(people_dirs)\n",
        "\n",
        "    train_end = int(train_ratio * len(people_dirs))\n",
        "    val_end = train_end + int(val_ratio * len(people_dirs))\n",
        "\n",
        "    train_dirs = people_dirs[:train_end]\n",
        "    val_dirs = people_dirs[train_end:val_end]\n",
        "    test_dirs = people_dirs[val_end:]\n",
        "\n",
        "    return train_dirs, val_dirs, test_dirs\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(project='face-dataset-project')\n",
        "\n",
        "# Hyperparameters and setup\n",
        "batch_size = 128\n",
        "learning_rate = 0.01\n",
        "epochs = 20\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Log hyperparameters to wandb\n",
        "wandb.config.update({\n",
        "    \"batch_size\": batch_size,\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"epochs\": epochs,\n",
        "    \"device\": str(device)\n",
        "})\n",
        "\n",
        "print(f'Batch size: {batch_size}')\n",
        "print(f'LR: {learning_rate}')\n",
        "print(f'Epochs: {epochs}')\n",
        "print(f'Device: {device}')\n",
        "\n",
        "# Data augmentation and normalization\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((112, 112)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Load dataset\n",
        "image_folder = 'generated_images_10Kids_cropped'  # Update with the path to your dataset\n",
        "train_dirs, val_dirs, test_dirs = split_dataset(image_folder, random_seed=69)\n",
        "\n",
        "train_dataset = FaceDataset(image_folder, train_dirs, transform=transform)\n",
        "val_dataset = FaceDataset(image_folder, val_dirs, transform=transform)\n",
        "test_dataset = FaceDataset(image_folder, test_dirs, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model = SiameseNetwork().to(device)\n",
        "summary(model, [(1, 112, 112), (1, 112, 112)])\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training script with validation\n",
        "def train(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\") as pbar:\n",
        "            for img1, img2, label in train_loader:\n",
        "                img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(img1, img2).squeeze()\n",
        "                loss = criterion(outputs, label)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "                pbar.set_postfix(loss=running_loss / (pbar.n + 1))\n",
        "                pbar.update(1)\n",
        "        \n",
        "        val_loss, val_accuracy = evaluate(model, val_loader, criterion)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {running_loss/len(train_loader)}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}\")\n",
        "\n",
        "        # Log metrics to wandb\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": running_loss / len(train_loader),\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_accuracy\": val_accuracy\n",
        "        })\n",
        "\n",
        "        # Save the model\n",
        "        torch.save(model.state_dict(), f'networks/network_epoch{epoch}.pth')\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for img1, img2, label in data_loader:\n",
        "            img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
        "            outputs = model(img1, img2).squeeze()\n",
        "            loss = criterion(outputs, label)\n",
        "            running_loss += loss.item()\n",
        "            predicted = (outputs > 0.5).float()\n",
        "            correct += (predicted == label).sum().item()\n",
        "            total += label.size(0)\n",
        "    accuracy = correct / total\n",
        "    return running_loss / len(data_loader), accuracy\n",
        "\n",
        "# Train the model\n",
        "train(model, train_loader, val_loader, criterion, optimizer, epochs=epochs)\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_accuracy = evaluate(model, test_loader, criterion)\n",
        "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n",
        "\n",
        "# Log final test metrics to wandb\n",
        "wandb.log({\n",
        "    \"test_loss\": test_loss,\n",
        "    \"test_accuracy\": test_accuracy\n",
        "})\n",
        "\n",
        "# Finish wandb run  \n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/nightly/cu121, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: torch in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (2.3.1)\n",
            "Requirement already satisfied: torchvision in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (0.18.1)\n",
            "Requirement already satisfied: torchaudio in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (2.3.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (from torch) (2021.4.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (from torchvision) (2.0.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.13.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\phili\\pythonprojects\\da2cs\\.venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "#!pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "Number of GPUs: 1\n",
            "GPU Name: NVIDIA GeForce RTX 3060\n"
          ]
        }
      ],
      "source": [
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
        "print(\"GPU Name:\", torch.cuda.get_device_name(0))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
