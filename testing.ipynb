{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from torchsummary import summary\n",
    "import pandas as pd\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing of Siamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=5, padding=0)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(32 * 12 * 12, 41)  # Updated to 32 * 12 * 12\n",
    "        self.fc1komma5 = nn.Linear(41,32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        x = F.relu(self.conv1(x)) # 8 * 112 * 112\n",
    "        x = F.max_pool2d(x, 2)  # output size: (8, 56, 56)\n",
    "        x = F.relu(self.conv2(x)) # 16* 52 * 52\n",
    "        x = F.max_pool2d(x, 2)  # output size: (16, 26, 26)\n",
    "        x = F.relu(self.conv3(x)) # 32 * 24 * 24\n",
    "        x = F.max_pool2d(x, 2)  # output size: (32, 12, 12)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc1komma5(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_one(input1)\n",
    "        output2 = self.forward_one(input2)\n",
    "        distance = torch.abs(output1 - output2)\n",
    "        output = self.fc3(distance)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Image file not found for pair 4. Returning None.\n",
      "Warning: Image file not found for pair 5608. Returning None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phili\\AppData\\Local\\Temp\\ipykernel_7880\\2228442471.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('golden_networks/networks-siamese/final_network.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mated Scores: [0.9458690285682678, 0.9680026173591614, 0.05446825921535492, 0.5319024324417114, 0.9370802044868469]\n",
      "Non-Mated Scores: [0.07858853042125702, 6.347186717903242e-05, 0.24736875295639038, 8.3736922533717e-05, 0.9689295291900635]\n",
      "Mated Accuracy: 56.37%\n",
      "Non-Mated Accuracy: 85.00%\n"
     ]
    }
   ],
   "source": [
    "class RealTestDataset(Dataset):\n",
    "    def __init__(self, image_folder, pairs_file, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.pairs = self._read_pairs(pairs_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def _read_pairs(self, pairs_file):\n",
    "        pairs = []\n",
    "        with open(pairs_file, 'r') as file:\n",
    "            for line in file:\n",
    "                name_1, number_1, name_2, number_2 = line.strip().split()\n",
    "                img1_path = os.path.join(self.image_folder, name_1, f'{name_1}_{int(number_1):04d}.png')\n",
    "                img2_path = os.path.join(self.image_folder, name_2, f'{name_2}_{int(number_2):04d}.png')\n",
    "                label = 1 if name_1 == name_2 else 0\n",
    "                pairs.append((img1_path.replace('\\\\', '/'), img2_path.replace('\\\\', '/'), label))\n",
    "        return pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img1_path, img2_path, label = self.pairs[idx]\n",
    "        try:\n",
    "            img1 = Image.open(img1_path).convert('L')\n",
    "            img2 = Image.open(img2_path).convert('L')\n",
    "        except FileNotFoundError:\n",
    "            # Handle missing image files\n",
    "            print(f\"Warning: Image file not found for pair {idx}. Returning None.\")\n",
    "            return None\n",
    "\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1) if img1 else None\n",
    "            img2 = self.transform(img2) if img2 else None\n",
    "\n",
    "        return img1, img2, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "# Define the transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Initialize the dataset and dataloader\n",
    "image_folder = 'lfw_cropped/lfw_cropped'  # Update with the path to your unzipped dataset folder\n",
    "pairs_file = 'pairs.txt'  # Update with the path to pairs.txt\n",
    "test_dataset = RealTestDataset(image_folder, pairs_file, transform=transform)\n",
    "\n",
    "# Filter out pairs with missing images and create DataLoader\n",
    "filtered_pairs = [pair for pair in test_dataset if pair is not None]\n",
    "filtered_dataset = [x for x in filtered_pairs if x is not None]\n",
    "test_loader = DataLoader(filtered_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Load the saved model weights\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SiameseNetwork().to(device)\n",
    "model.load_state_dict(torch.load('golden_networks/networks-siamese/final_network.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    mated_scores = []\n",
    "    non_mated_scores = []\n",
    "    with torch.no_grad():\n",
    "        for img1, img2, label in data_loader:\n",
    "            if img1 is None or img2 is None:\n",
    "                # Handle missing images: Skip or assign a placeholder score\n",
    "                continue\n",
    "            \n",
    "            img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "            outputs = model(img1, img2).squeeze()\n",
    "            score = torch.sigmoid(outputs).item()\n",
    "            if label.item() == 1:\n",
    "                mated_scores.append(score)\n",
    "            else:\n",
    "                non_mated_scores.append(score)\n",
    "    \n",
    "    # Pad the shorter list with -1 for uneven lengths (optional)\n",
    "    max_len = max(len(mated_scores), len(non_mated_scores))\n",
    "    mated_scores.extend([-1] * (max_len - len(mated_scores)))\n",
    "    non_mated_scores.extend([-1] * (max_len - len(non_mated_scores)))\n",
    "    \n",
    "    return mated_scores, non_mated_scores\n",
    "\n",
    "# Evaluate on the real test set\n",
    "mated_scores, non_mated_scores = evaluate(model, test_loader)\n",
    "\n",
    "# Save mated and non-mated scores to .txt files\n",
    "mated_scores_file = 'siamesescores/mated_scores.txt'\n",
    "non_mated_scores_file = 'siamesescores/non_mated_scores.txt'\n",
    "\n",
    "with open(mated_scores_file, 'w') as f:\n",
    "    for score in mated_scores:\n",
    "        f.write(f'{score}\\n')\n",
    "\n",
    "with open(non_mated_scores_file, 'w') as f:\n",
    "    for score in non_mated_scores:\n",
    "        f.write(f'{score}\\n')\n",
    "\n",
    "print(f'Mated Scores: {mated_scores[:5]}')\n",
    "print(f'Non-Mated Scores: {non_mated_scores[:5]}')\n",
    "\n",
    "with zipfile.ZipFile('siamesescores/predictions.zip', 'w') as zipf:\n",
    "    zipf.write(mated_scores_file)\n",
    "    zipf.write(non_mated_scores_file)\n",
    "    \n",
    "# Calculate accuracy for mated scores (> 0.5)\n",
    "mated_correct = sum(score > 0.5 for score in mated_scores)\n",
    "mated_accuracy = mated_correct / len(mated_scores) * 100\n",
    "\n",
    "# Calculate accuracy for non-mated scores (< 0.5)\n",
    "non_mated_correct = sum(score < 0.5 for score in non_mated_scores)\n",
    "non_mated_accuracy = non_mated_correct / len(non_mated_scores) * 100\n",
    "\n",
    "print(f\"Mated Accuracy: {mated_accuracy:.2f}%\")\n",
    "print(f\"Non-Mated Accuracy: {non_mated_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing of Triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=5, padding=0)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(32 * 12 * 12, 41)  # Updated to 32 * 12 * 12\n",
    "        self.fc1komma5 = nn.Linear(41,32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        x = F.relu(self.conv1(x)) # 8 * 112 * 112\n",
    "        x = F.max_pool2d(x, 2)  # output size: (8, 56, 56)\n",
    "        x = F.relu(self.conv2(x)) # 16* 52 * 52\n",
    "        x = F.max_pool2d(x, 2)  # output size: (16, 26, 26)\n",
    "        x = F.relu(self.conv3(x)) # 32 * 24 * 24\n",
    "        x = F.max_pool2d(x, 2)  # output size: (32, 12, 12)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc1komma5(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2, input3):\n",
    "        output1 = self.forward_one(input1)\n",
    "        output2 = self.forward_one(input2)\n",
    "        output3 = self.forward_one(input3)\n",
    "        return output1, output2, output3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phili\\AppData\\Local\\Temp\\ipykernel_7880\\2571125585.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('triplet/network_epoch1.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 6000 pairs...\n",
      "Warning: Image file not found for pair 4. Returning placeholder tensors.\n",
      "Progress: 100/6000\n",
      "Progress: 200/6000\n",
      "Progress: 300/6000\n",
      "Progress: 400/6000\n",
      "Progress: 500/6000\n",
      "Progress: 600/6000\n",
      "Progress: 700/6000\n",
      "Progress: 800/6000\n",
      "Progress: 900/6000\n",
      "Progress: 1000/6000\n",
      "Progress: 1100/6000\n",
      "Progress: 1200/6000\n",
      "Progress: 1300/6000\n",
      "Progress: 1400/6000\n",
      "Progress: 1500/6000\n",
      "Progress: 1600/6000\n",
      "Progress: 1700/6000\n",
      "Progress: 1800/6000\n",
      "Progress: 1900/6000\n",
      "Progress: 2000/6000\n",
      "Progress: 2100/6000\n",
      "Progress: 2200/6000\n",
      "Progress: 2300/6000\n",
      "Progress: 2400/6000\n",
      "Progress: 2500/6000\n",
      "Progress: 2600/6000\n",
      "Progress: 2700/6000\n",
      "Progress: 2800/6000\n",
      "Progress: 2900/6000\n",
      "Progress: 3000/6000\n",
      "Progress: 3100/6000\n",
      "Progress: 3200/6000\n",
      "Progress: 3300/6000\n",
      "Progress: 3400/6000\n",
      "Progress: 3500/6000\n",
      "Progress: 3600/6000\n",
      "Progress: 3700/6000\n",
      "Progress: 3800/6000\n",
      "Progress: 3900/6000\n",
      "Progress: 4000/6000\n",
      "Progress: 4100/6000\n",
      "Progress: 4200/6000\n",
      "Progress: 4300/6000\n",
      "Progress: 4400/6000\n",
      "Progress: 4500/6000\n",
      "Progress: 4600/6000\n",
      "Progress: 4700/6000\n",
      "Progress: 4800/6000\n",
      "Progress: 4900/6000\n",
      "Progress: 5000/6000\n",
      "Progress: 5100/6000\n",
      "Progress: 5200/6000\n",
      "Progress: 5300/6000\n",
      "Progress: 5400/6000\n",
      "Progress: 5500/6000\n",
      "Progress: 5600/6000\n",
      "Warning: Image file not found for pair 5608. Returning placeholder tensors.\n",
      "Progress: 5700/6000\n",
      "Progress: 5800/6000\n",
      "Progress: 5900/6000\n",
      "Progress: 6000/6000\n",
      "Mated Scores: [0.8409874439239502, 0.6676175594329834, 0.6955742239952087, 0.6155218482017517, 0.6538993716239929]\n",
      "Non-Mated Scores: [0.9999994039535522, 0.4444238543510437, 0.2771148383617401, 0.4203175902366638, 0.27027878165245056]\n",
      "Predictions saved to tripletscores/predictions.zip\n",
      "Mated Accuracy: 74.19%\n",
      "Non-Mated Accuracy: 61.81%\n",
      "Mean Absolute Error (MAE): 0.4365\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "device = torch.device(\"cpu\")\n",
    "model = SiameseNetwork().to(device)\n",
    "model.load_state_dict(torch.load('triplet/network_epoch1.pth'))\n",
    "model.eval()\n",
    " \n",
    "# Define the transformation (should match the transformation used during training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    " \n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert('L')\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0)  # Add batch dimension\n",
    "    return image\n",
    " \n",
    "def compute_similarity(model, image1, image2, alpha=0.155):\n",
    "    with torch.no_grad():\n",
    "        embedding1 = model.forward_one(image1)\n",
    "        embedding2 = model.forward_one(image2)\n",
    "        distance = torch.mean(F.pairwise_distance(embedding1, embedding2))\n",
    "        similarity = torch.exp(-alpha * distance)\n",
    "        #similarity = 1 / (1 + torch.exp(alpha * distance))\n",
    "    return similarity.item()\n",
    "    #return distance\n",
    " \n",
    "\n",
    "class RealTestDataset(Dataset):\n",
    "    def __init__(self, image_folder, pairs_file, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.pairs = self._read_pairs(pairs_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def _read_pairs(self, pairs_file):\n",
    "        pairs = []\n",
    "        with open(pairs_file, 'r') as file:\n",
    "            for line in file:\n",
    "                name_1, number_1, name_2, number_2 = line.strip().split()\n",
    "                img1_path = os.path.join(self.image_folder, name_1, f'{name_1}_{int(number_1):04d}.png')\n",
    "                img2_path = os.path.join(self.image_folder, name_2, f'{name_2}_{int(number_2):04d}.png')\n",
    "                label = 1 if name_1 == name_2 else 0\n",
    "                pairs.append((img1_path.replace('\\\\', '/'), img2_path.replace('\\\\', '/'), label))\n",
    "        return pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img1_path, img2_path, label = self.pairs[idx]\n",
    "        try:\n",
    "            img1 = Image.open(img1_path).convert('L')\n",
    "            img2 = Image.open(img2_path).convert('L')\n",
    "        except FileNotFoundError:\n",
    "            # Handle missing image files\n",
    "            print(f\"Warning: Image file not found for pair {idx}. Returning placeholder tensors.\")\n",
    "            img1 = Image.new('L', (112, 112))  # Create a black image placeholder\n",
    "            img2 = Image.new('L', (112, 112))  # Create a black image placeholder\n",
    "\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        return img1, img2, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Define the transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Initialize the dataset and dataloader\n",
    "image_folder = 'lfw_cropped/lfw_cropped'  # Update with the path to your unzipped dataset folder\n",
    "pairs_file = 'pairs.txt'  # Update with the path to pairs.txt\n",
    "test_dataset = RealTestDataset(image_folder, pairs_file, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    mated_scores = []\n",
    "    non_mated_scores = []\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    total_pairs = len(data_loader.dataset)\n",
    "\n",
    "    print(f\"Evaluating {total_pairs} pairs...\")\n",
    "    for idx, (img1, img2, label) in enumerate(data_loader):\n",
    "        if img1 is None or img2 is None:\n",
    "            mated_scores.append(-1)\n",
    "            non_mated_scores.append(-1)\n",
    "            continue\n",
    "        \n",
    "        img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "        # Compute similarity\n",
    "        score = compute_similarity(model, img1, img2)\n",
    "        \n",
    "        all_scores.append(score)\n",
    "        all_labels.append(label.item())\n",
    "\n",
    "        if label.item() == 1:\n",
    "            mated_scores.append(score)\n",
    "        else:\n",
    "            non_mated_scores.append(score)\n",
    "\n",
    "        # Print progress\n",
    "        if (idx + 1) % 100 == 0 or (idx + 1) == total_pairs:\n",
    "            print(f\"Progress: {idx + 1}/{total_pairs}\")\n",
    "\n",
    "    return mated_scores, non_mated_scores, all_scores, all_labels\n",
    "\n",
    "# Evaluate on the real test set specified in pairs.txt\n",
    "mated_scores, non_mated_scores, all_scores, all_labels = evaluate(model, test_loader)\n",
    "\n",
    "# Save mated and non-mated scores to .txt files\n",
    "mated_scores_file = 'tripletscores/mated_scores.txt'\n",
    "non_mated_scores_file = 'tripletscores/non_mated_scores.txt'\n",
    "\n",
    "with open(mated_scores_file, 'w') as f:\n",
    "    for score in mated_scores:\n",
    "        f.write(f'{score}\\n')\n",
    "\n",
    "with open(non_mated_scores_file, 'w') as f:\n",
    "    for score in non_mated_scores:\n",
    "        f.write(f'{score}\\n')\n",
    "\n",
    "print(f'Mated Scores: {mated_scores[:5]}')\n",
    "print(f'Non-Mated Scores: {non_mated_scores[:5]}')\n",
    "\n",
    "# Zip the files\n",
    "zip_filename = 'tripletscores/predictions.zip'\n",
    "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "    zipf.write(mated_scores_file)\n",
    "    zipf.write(non_mated_scores_file)\n",
    "\n",
    "print(f\"Predictions saved to {zip_filename}\")\n",
    "\n",
    "# Calculate accuracy for mated scores (> 0.5)\n",
    "mated_correct = sum(score > 0.5 for score in mated_scores)\n",
    "mated_accuracy = mated_correct / len(mated_scores) * 100\n",
    "\n",
    "# Calculate accuracy for non-mated scores (< 0.5)\n",
    "non_mated_correct = sum(score < 0.5 for score in non_mated_scores)\n",
    "non_mated_accuracy = non_mated_correct / len(non_mated_scores) * 100\n",
    "\n",
    "print(f\"Mated Accuracy: {mated_accuracy:.2f}%\")\n",
    "print(f\"Non-Mated Accuracy: {non_mated_accuracy:.2f}%\")\n",
    "\n",
    "# Calculate mean absolute error (MAE)\n",
    "mae = sum(abs(score - label) for score, label in zip(all_scores, all_labels)) / len(all_labels)\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Quadruplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=5, padding=0)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(32 * 12 * 12, 41)  # Updated to 32 * 12 * 12\n",
    "        self.fc1komma5 = nn.Linear(41,32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        x = F.relu(self.conv1(x)) # 8 * 112 * 112\n",
    "        x = F.max_pool2d(x, 2)  # output size: (8, 56, 56)\n",
    "        x = F.relu(self.conv2(x)) # 16* 52 * 52\n",
    "        x = F.max_pool2d(x, 2)  # output size: (16, 26, 26)\n",
    "        x = F.relu(self.conv3(x)) # 32 * 24 * 24\n",
    "        x = F.max_pool2d(x, 2)  # output size: (32, 12, 12)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc1komma5(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2, input3, input4):\n",
    "        output1 = self.forward_one(input1)\n",
    "        output2 = self.forward_one(input2)\n",
    "        output3 = self.forward_one(input3)\n",
    "        output4 = self.forward_one(input4)\n",
    "        return output1, output2, output3, output4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phili\\AppData\\Local\\Temp\\ipykernel_7880\\506473446.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('networks_christoph_quadro/network_epoch10.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 6000 pairs...\n",
      "Warning: Image file not found for pair 4. Returning placeholder tensors.\n",
      "Progress: 100/6000\n",
      "Progress: 200/6000\n",
      "Progress: 300/6000\n",
      "Progress: 400/6000\n",
      "Progress: 500/6000\n",
      "Progress: 600/6000\n",
      "Progress: 700/6000\n",
      "Progress: 800/6000\n",
      "Progress: 900/6000\n",
      "Progress: 1000/6000\n",
      "Progress: 1100/6000\n",
      "Progress: 1200/6000\n",
      "Progress: 1300/6000\n",
      "Progress: 1400/6000\n",
      "Progress: 1500/6000\n",
      "Progress: 1600/6000\n",
      "Progress: 1700/6000\n",
      "Progress: 1800/6000\n",
      "Progress: 1900/6000\n",
      "Progress: 2000/6000\n",
      "Progress: 2100/6000\n",
      "Progress: 2200/6000\n",
      "Progress: 2300/6000\n",
      "Progress: 2400/6000\n",
      "Progress: 2500/6000\n",
      "Progress: 2600/6000\n",
      "Progress: 2700/6000\n",
      "Progress: 2800/6000\n",
      "Progress: 2900/6000\n",
      "Progress: 3000/6000\n",
      "Progress: 3100/6000\n",
      "Progress: 3200/6000\n",
      "Progress: 3300/6000\n",
      "Progress: 3400/6000\n",
      "Progress: 3500/6000\n",
      "Progress: 3600/6000\n",
      "Progress: 3700/6000\n",
      "Progress: 3800/6000\n",
      "Progress: 3900/6000\n",
      "Progress: 4000/6000\n",
      "Progress: 4100/6000\n",
      "Progress: 4200/6000\n",
      "Progress: 4300/6000\n",
      "Progress: 4400/6000\n",
      "Progress: 4500/6000\n",
      "Progress: 4600/6000\n",
      "Progress: 4700/6000\n",
      "Progress: 4800/6000\n",
      "Progress: 4900/6000\n",
      "Progress: 5000/6000\n",
      "Progress: 5100/6000\n",
      "Progress: 5200/6000\n",
      "Progress: 5300/6000\n",
      "Progress: 5400/6000\n",
      "Progress: 5500/6000\n",
      "Progress: 5600/6000\n",
      "Warning: Image file not found for pair 5608. Returning placeholder tensors.\n",
      "Progress: 5700/6000\n",
      "Progress: 5800/6000\n",
      "Progress: 5900/6000\n",
      "Progress: 6000/6000\n",
      "Mated Scores: [0.7191744446754456, 0.837295413017273, 0.5573331117630005, 0.4919019639492035, 0.4819362163543701]\n",
      "Non-Mated Scores: [0.9999994039535522, 0.39596474170684814, 0.3248435854911804, 0.5049030780792236, 0.282503604888916]\n",
      "Predictions saved to quadruplescores/predictions.zip\n",
      "Mated Accuracy: 70.36%\n",
      "Non-Mated Accuracy: 70.18%\n",
      "Mean Absolute Error (MAE): 0.4318\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "device = torch.device(\"cpu\")\n",
    "model = SiameseNetwork().to(device)\n",
    "model.load_state_dict(torch.load('networks_christoph_quadro/network_epoch10.pth'))\n",
    "model.eval()\n",
    " \n",
    "# Define the transformation (should match the transformation used during training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    " \n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert('L')\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0)  # Add batch dimension\n",
    "    return image\n",
    " \n",
    "def compute_similarity(model, image1, image2, alpha=0.155):\n",
    "    with torch.no_grad():\n",
    "        embedding1 = model.forward_one(image1)\n",
    "        embedding2 = model.forward_one(image2)\n",
    "        distance = torch.mean(F.pairwise_distance(embedding1, embedding2))\n",
    "        similarity = torch.exp(-alpha * distance)\n",
    "        #similarity = 1 / (1 + torch.exp(alpha * distance))\n",
    "    return similarity.item()\n",
    "    #return distance\n",
    " \n",
    "\n",
    "class RealTestDataset(Dataset):\n",
    "    def __init__(self, image_folder, pairs_file, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.pairs = self._read_pairs(pairs_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def _read_pairs(self, pairs_file):\n",
    "        pairs = []\n",
    "        with open(pairs_file, 'r') as file:\n",
    "            for line in file:\n",
    "                name_1, number_1, name_2, number_2 = line.strip().split()\n",
    "                img1_path = os.path.join(self.image_folder, name_1, f'{name_1}_{int(number_1):04d}.png')\n",
    "                img2_path = os.path.join(self.image_folder, name_2, f'{name_2}_{int(number_2):04d}.png')\n",
    "                label = 1 if name_1 == name_2 else 0\n",
    "                pairs.append((img1_path.replace('\\\\', '/'), img2_path.replace('\\\\', '/'), label))\n",
    "        return pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img1_path, img2_path, label = self.pairs[idx]\n",
    "        try:\n",
    "            img1 = Image.open(img1_path).convert('L')\n",
    "            img2 = Image.open(img2_path).convert('L')\n",
    "        except FileNotFoundError:\n",
    "            # Handle missing image files\n",
    "            print(f\"Warning: Image file not found for pair {idx}. Returning placeholder tensors.\")\n",
    "            img1 = Image.new('L', (112, 112))  # Create a black image placeholder\n",
    "            img2 = Image.new('L', (112, 112))  # Create a black image placeholder\n",
    "\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        return img1, img2, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Define the transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Initialize the dataset and dataloader\n",
    "image_folder = 'lfw_cropped/lfw_cropped'  # Update with the path to your unzipped dataset folder\n",
    "pairs_file = 'pairs.txt'  # Update with the path to pairs.txt\n",
    "test_dataset = RealTestDataset(image_folder, pairs_file, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    mated_scores = []\n",
    "    non_mated_scores = []\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    total_pairs = len(data_loader.dataset)\n",
    "\n",
    "    print(f\"Evaluating {total_pairs} pairs...\")\n",
    "    for idx, (img1, img2, label) in enumerate(data_loader):\n",
    "        if img1 is None or img2 is None:\n",
    "            mated_scores.append(-1)\n",
    "            non_mated_scores.append(-1)\n",
    "            continue\n",
    "        \n",
    "        img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "        # Compute similarity\n",
    "        score = compute_similarity(model, img1, img2)\n",
    "        \n",
    "        all_scores.append(score)\n",
    "        all_labels.append(label.item())\n",
    "\n",
    "        if label.item() == 1:\n",
    "            mated_scores.append(score)\n",
    "        else:\n",
    "            non_mated_scores.append(score)\n",
    "\n",
    "        # Print progress\n",
    "        if (idx + 1) % 100 == 0 or (idx + 1) == total_pairs:\n",
    "            print(f\"Progress: {idx + 1}/{total_pairs}\")\n",
    "\n",
    "    return mated_scores, non_mated_scores, all_scores, all_labels\n",
    "\n",
    "# Evaluate on the real test set specified in pairs.txt\n",
    "mated_scores, non_mated_scores, all_scores, all_labels = evaluate(model, test_loader)\n",
    "\n",
    "# Save mated and non-mated scores to .txt files\n",
    "mated_scores_file = 'quadruplescores/mated_scores.txt'\n",
    "non_mated_scores_file = 'quadruplescores/non_mated_scores.txt'\n",
    "\n",
    "with open(mated_scores_file, 'w') as f:\n",
    "    for score in mated_scores:\n",
    "        f.write(f'{score}\\n')\n",
    "\n",
    "with open(non_mated_scores_file, 'w') as f:\n",
    "    for score in non_mated_scores:\n",
    "        f.write(f'{score}\\n')\n",
    "\n",
    "print(f'Mated Scores: {mated_scores[:5]}')\n",
    "print(f'Non-Mated Scores: {non_mated_scores[:5]}')\n",
    "\n",
    "# Zip the files\n",
    "zip_filename = 'quadruplescores/predictions.zip'\n",
    "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "    zipf.write(mated_scores_file)\n",
    "    zipf.write(non_mated_scores_file)\n",
    "\n",
    "print(f\"Predictions saved to {zip_filename}\")\n",
    "\n",
    "# Calculate accuracy for mated scores (> 0.5)\n",
    "mated_correct = sum(score > 0.5 for score in mated_scores)\n",
    "mated_accuracy = mated_correct / len(mated_scores) * 100\n",
    "\n",
    "# Calculate accuracy for non-mated scores (< 0.5)\n",
    "non_mated_correct = sum(score < 0.5 for score in non_mated_scores)\n",
    "non_mated_accuracy = non_mated_correct / len(non_mated_scores) * 100\n",
    "\n",
    "print(f\"Mated Accuracy: {mated_accuracy:.2f}%\")\n",
    "print(f\"Non-Mated Accuracy: {non_mated_accuracy:.2f}%\")\n",
    "\n",
    "# Calculate mean absolute error (MAE)\n",
    "mae = sum(abs(score - label) for score, label in zip(all_scores, all_labels)) / len(all_labels)\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
